{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) What is a Convolutional Neural Network (CNN), and why is it used for image processing?\n",
        "**Ans:** A **Convolutional Neural Network (CNN)** is a type of deep learning model specifically designed to process structured grid-like data, such as images. It is particularly effective for image processing tasks due to its ability to automatically and adaptively learn spatial hierarchies of features from the input data.\n",
        "\n",
        "### Why CNNs are Used for Image Processing:\n",
        "1. **Spatial Hierarchy**:  \n",
        "   CNNs learn local features at initial layers (like edges or textures) and progressively abstract these into higher-level features (like objects or faces) in deeper layers.\n",
        "\n",
        "2. **Parameter Sharing**:  \n",
        "   Convolutional layers use the same filter across the entire image, reducing the number of parameters compared to fully connected networks.\n",
        "\n",
        "3. **Translation Invariance**:  \n",
        "   Pooling layers and shared weights make CNNs robust to small shifts, rotations, and distortions in images.\n",
        "\n",
        "4. **Automatic Feature Extraction**:  \n",
        "   CNNs do not require hand-engineered features. They learn directly from raw image data, reducing manual effort.\n",
        "\n",
        "5. **Efficiency**:  \n",
        "   Due to parameter sharing and sparsity of connections, CNNs are computationally efficient and scalable to high-dimensional data like images.\n",
        "\n",
        "# 2) What are the key components of a CNN architecture?\n",
        "**Ans:** The key components of a Convolutional Neural Network (CNN) architecture are as follows:\n",
        "\n",
        "1. **Input Layer**:\n",
        "   - **Function**: Receives the raw input data, typically an image represented as a multi-dimensional array (e.g., height × width × channels for RGB images).\n",
        "   - **Details**: This layer standardizes the input data, often normalizing pixel values to a specific range to facilitate efficient processing by subsequent layers.\n",
        "\n",
        "2. **Convolutional Layers**:\n",
        "   - **Function**: Apply convolution operations to the input data, using filters (kernels) to detect local patterns such as edges, textures, and more complex features.\n",
        "   - **Details**: Each filter convolves across the input, producing a feature map that highlights the presence of specific features at various spatial locations. The use of multiple filters allows the network to learn a diverse set of features.\n",
        "\n",
        "3. **Activation Functions**:\n",
        "   - **Function**: Introduce non-linearities into the network, enabling it to capture complex patterns and relationships.\n",
        "   - **Details**: Common activation functions include ReLU (Rectified Linear Unit), which outputs zero for negative inputs and the input itself for positive inputs, and others like sigmoid or tanh, though ReLU is most prevalent in CNNs.\n",
        "\n",
        "4. **Pooling (Subsampling) Layers**:\n",
        "   - **Function**: Reduce the spatial dimensions of feature maps, decreasing computational load and controlling overfitting by summarizing feature information.\n",
        "   - **Details**: Pooling operations, such as max pooling (selecting the maximum value in a region) or average pooling (computing the average value), are applied over non-overlapping regions of the feature map.\n",
        "\n",
        "5. **Fully Connected (Dense) Layers**:\n",
        "   - **Function**: Integrate features learned by convolutional and pooling layers to perform high-level reasoning and make final predictions.\n",
        "   - **Details**: In these layers, each neuron is connected to every neuron in the previous layer, allowing for the combination of features across the entire image to classify or regress outputs.\n",
        "\n",
        "6. **Dropout Layers (Optional)**:\n",
        "   - **Function**: Mitigate overfitting by randomly setting a fraction of input units to zero during training, preventing the network from becoming overly reliant on specific neurons.\n",
        "   - **Details**: Dropout is applied during training and is typically deactivated during evaluation and testing phases.\n",
        "\n",
        "7. **Output Layer**:\n",
        "   - **Function**: Produce the final output of the network, corresponding to the desired prediction task.\n",
        "   - **Details**: For classification tasks, this layer often uses a softmax activation function to output a probability distribution over classes; for regression tasks, it may use a linear activation function to produce continuous values.\n",
        "\n",
        "# 3) What is the role of the convolutional layer in CNNs?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), the convolutional layer serves as a fundamental building block responsible for feature extraction from input data, such as images. This layer applies a set of learnable filters (kernels) to the input, performing convolution operations that produce feature maps. These feature maps highlight various aspects of the input, enabling the network to detect patterns like edges, textures, and more complex structures.\n",
        "# 4) What is a filter (kernel) in CNNs?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), a filter (also known as a kernel) is a small matrix of learnable weights used to detect specific features in the input data, such as edges, textures, or patterns. During the convolution operation, this filter slides over the input data, performing element-wise multiplications and summing the results to produce a feature map. This process enables the network to capture spatial hierarchies and learn representations essential for tasks like image recognition and classification.\n",
        "\n",
        "# 5) What is pooling in CNNs, and why is it important?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), pooling—also known as subsampling or downsampling—is a crucial operation that reduces the spatial dimensions (height and width) of feature maps while preserving their depth. This process simplifies the computational complexity of the network and aids in extracting dominant features, making the model more robust to variations and distortions in the input data.\n",
        "\n",
        "# 6) What are the common types of pooling used in CNNs?\n",
        "**Ans:** The most common types of pooling used in CNNs include:\n",
        "- Max Pooling:\n",
        "\n",
        "    - Selects the maximum value within a defined window (e.g., 2×2) as it moves across the input feature map. This method emphasizes the most prominent features detected in the previous layers.\n",
        "\n",
        "- Average Pooling:\n",
        "\n",
        "    - Computes the average of all values within the pooling window, providing a smoother representation of the feature map.\n",
        "\n",
        "# 7) How does the backpropagation algorithm work in CNNs?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), the **backpropagation algorithm** is essential for training the network by adjusting its weights and biases to minimize the error between predicted and actual outputs. This process involves propagating the error backward through the network to update the parameters of both convolutional and fully connected layers.\n",
        "\n",
        "**How Backpropagation Works in CNNs:**\n",
        "\n",
        "1. **Forward Pass:**\n",
        "   - The input data passes through the network's layers, undergoing operations like convolution, activation, pooling, and fully connected transformations, to produce an output.\n",
        "\n",
        "2. **Loss Calculation:**\n",
        "   - A loss function computes the difference between the network's output and the actual target values, quantifying the prediction error.\n",
        "\n",
        "3. **Backward Pass (Backpropagation):**\n",
        "   - The error is propagated backward through the network to compute gradients of the loss function with respect to each parameter.\n",
        "   - **Convolutional Layers:**\n",
        "     - Gradients with respect to the filters (kernels) and input feature maps are calculated.\n",
        "     - These gradients are used to update the filters, enabling the network to learn the most relevant features for the task.\n",
        "   - **Fully Connected Layers:**\n",
        "     - Gradients with respect to weights and biases are computed similarly to traditional neural networks.\n",
        "\n",
        "4. **Parameter Update:**\n",
        "   - Using the computed gradients, optimization algorithms (e.g., stochastic gradient descent) adjust the network's parameters to minimize the loss function iteratively.\n",
        "\n",
        "# 8) What is the role of activation functions in CNNs?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), activation functions are pivotal in introducing non-linearities into the model, enabling it to learn and represent complex patterns within the data. Without these non-linear activation functions, a neural network, regardless of its depth, would behave as a linear model, limiting its capacity to capture intricate relationships.\n",
        "\n",
        "# 9) What is the concept of receptive fields in CNNs?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), the receptive field refers to the specific region of the input data that a particular neuron or feature map in a given layer \"sees\" or responds to. In other words, it is the portion of the input image that influences the activation of a neuron in a specific layer. Understanding receptive fields is crucial for designing CNN architectures that effectively capture spatial hierarchies and patterns in data.\n",
        "\n",
        "# 10) Explain the concept of tensor space in CNNs?\n",
        "**Ans:** In Convolutional Neural Networks (CNNs), the term tensor space refers to the multi-dimensional arrays, or tensors, that represent data and parameters within the network. These tensors are fundamental in handling the complex structures of input data, such as images, and in managing the weights and activations throughout the network's layers.\n",
        "\n",
        "# 11) What is LeNet-5, and how does it contribute to the development of CNNs?\n",
        "**Ans:** LeNet-5 is a pioneering convolutional neural network (CNN) architecture developed by Yann LeCun and his colleagues in 1998. It was specifically designed for handwritten and machine-printed character recognition, particularly for tasks like reading zip codes and digits.\n",
        "\n",
        "**Contributions to CNN Development:**\n",
        "\n",
        "- **Introduction of Convolutional Layers:** LeNet-5 was among the first architectures to utilize convolutional layers, enabling the network to automatically learn spatial hierarchies of features from input images.\n",
        "\n",
        "- **Use of Subsampling (Pooling) Layers:** The incorporation of subsampling layers (average pooling) helped in reducing the spatial dimensions of feature maps, leading to lower computational complexity and providing a form of translation invariance.\n",
        "\n",
        "- **Hierarchical Feature Extraction:** LeNet-5 demonstrated how multiple layers could be stacked to extract increasingly complex features, laying the groundwork for deeper and more sophisticated CNN architectures.\n",
        "\n",
        "- **Practical Application:** Its success in handwritten digit recognition showcased the practical viability of CNNs in real-world applications, influencing subsequent research and development in the field.\n",
        "\n",
        "# 12) What is AlexNet, and why was it a breakthrough in deep learning?\n",
        "**Ans:** AlexNet is a groundbreaking convolutional neural network (CNN) architecture introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. It was designed to perform image classification tasks.\n",
        "\n",
        "**Significance and Breakthroughs:**\n",
        "\n",
        "- **Superior Performance:** AlexNet significantly outperformed previous models, achieving a top-5 error rate of 15.3% in the 2012 ImageNet competition, compared to the 26.2% error rate of the second-best entry.\n",
        "\n",
        "\n",
        "- **GPU Utilization:** The implementation of AlexNet leveraged Graphics Processing Units (GPUs) for training, substantially reducing computation time and demonstrating the effectiveness of GPUs in deep learning tasks.\n",
        "\n",
        "\n",
        "- **Revival of Neural Networks:** The success of AlexNet played a pivotal role in reviving interest in neural networks and deep learning, leading to widespread adoption and further research in the field.\n",
        "\n",
        "\n",
        "- **Advancements in Computer Vision:** By demonstrating the efficacy of deep CNNs in image recognition, AlexNet paved the way for the development of more sophisticated architectures and applications in computer vision.\n",
        "\n",
        "# 13) What is VGGNet, and how does it differ from AlexNet?\n",
        "**Ans:** VGGNet is a convolutional neural network (CNN) architecture developed by the Visual Geometry Group at the University of Oxford. It is renowned for its depth, with versions like VGG-16 and VGG-19 containing 16 and 19 weight layers, respectively.\n",
        "\n",
        "**Differences Between VGGNet and AlexNet:**\n",
        "\n",
        "- **Filter Sizes:** AlexNet uses larger filters in its initial layers (e.g., 11×11 and 5×5), whereas VGGNet exclusively uses smaller 3×3 filters, stacked sequentially to achieve a larger receptive field.\n",
        "\n",
        "- **Depth:** VGGNet is deeper than AlexNet, with VGG-16 having 16 layers compared to AlexNet's 8 layers.\n",
        "\n",
        "- **Performance:** The increased depth and smaller filters of VGGNet enable it to learn more complex features, leading to improved performance on image classification benchmarks.\n",
        "\n",
        "- **Parameter Count:** VGGNet has a higher number of parameters compared to AlexNet, resulting in increased computational requirements and training time.\n",
        "\n",
        "# 14) What is GoogLeNet, and what is its main innovation?\n",
        "**Ans:** GoogLeNet, introduced by researchers at Google in 2014, is a deep convolutional neural network (CNN) architecture that marked a significant advancement in deep learning, particularly in computer vision tasks.\n",
        "\n",
        "**Main Innovation: The Inception Module**\n",
        "\n",
        "The core innovation of GoogLeNet is the Inception module, which allows the network to capture features at multiple scales simultaneously. Traditional CNN architectures often faced challenges in choosing the optimal kernel size for convolutions, as different sizes can capture different features. The Inception module addresses this by performing parallel convolutions with multiple kernel sizes (e.g., 1×1, 3×3, 5×5) within the same layer, and then concatenating the results. This design enables the network to process and learn from features of varying sizes and complexities concurrently.\n",
        "\n",
        "# 15) What is ResNet, and what problem does it solve?\n",
        "**Ans:** ResNet, short for **Residual Network**, is a deep learning architecture introduced by Kaiming He and colleagues in 2015. It was developed to address challenges associated with training very deep neural networks, particularly the **vanishing gradient problem** and the **degradation problem**.\n",
        "\n",
        "**Problems Addressed by ResNet:**\n",
        "\n",
        "- **Vanishing Gradient Problem:** In deep networks, gradients can become exceedingly small during backpropagation, hindering effective weight updates and slowing or halting learning. ResNet mitigates this issue by introducing residual connections that allow gradients to flow more directly through the network, facilitating the training of deeper architectures.\n",
        "\n",
        "- **Degradation Problem:** As networks deepen, adding more layers can lead to higher training errors, contrary to expectations. ResNet addresses this by enabling layers to learn residual functions relative to the input, effectively allowing the network to bypass unnecessary layers and focus on learning meaningful transformations.\n",
        "\n",
        "# 16) What is DenseNet, and how does it differ from ResNet?\n",
        "**Ans:** DenseNet, short for **Densely Connected Convolutional Network**, is a deep learning architecture introduced to enhance information flow and gradient propagation in neural networks. Unlike traditional architectures, DenseNet establishes direct connections between each layer and all its preceding layers within a dense block. This design ensures that feature maps from earlier layers are concatenated with those of subsequent layers, promoting feature reuse and efficient gradient flow.\n",
        "\n",
        "**Key Differences Between DenseNet and ResNet:**\n",
        "\n",
        "- **Connection Mechanism:**\n",
        "  - *ResNet:* Employs **skip (or residual) connections** where the output of a layer is added to the output of a deeper layer. This additive identity transformation helps mitigate the vanishing gradient problem by allowing gradients to bypass certain layers during backpropagation.\n",
        "  - *DenseNet:* Utilizes **dense connections**, concatenating the outputs (feature maps) of all preceding layers as inputs to subsequent layers. This concatenative approach ensures maximum information flow between layers.\n",
        "\n",
        "- **Feature Utilization:**\n",
        "  - *ResNet:* Each layer primarily accesses the output of the immediately preceding layer, with the residual connection providing an alternative pathway for gradient flow.\n",
        "  - *DenseNet:* Each layer has direct access to the feature maps of all preceding layers, leading to extensive feature reuse and potentially more efficient learning.\n",
        "\n",
        "- **Parameter Efficiency:**\n",
        "  - *ResNet:* The use of additive identity transformations can lead to a higher parameter count as the network depth increases.\n",
        "  - *DenseNet:* By reusing features through dense connections, DenseNet can achieve comparable or even superior performance with fewer parameters, enhancing computational efficiency.\n",
        "\n",
        "- **Computational Considerations:**\n",
        "  - *ResNet:* Generally requires less memory during training compared to DenseNet, as it doesn't need to store as many intermediate feature maps.\n",
        "  - *DenseNet:* The concatenation of feature maps from all preceding layers can increase memory usage, which may be a consideration in resource-constrained environments.\n",
        "\n",
        "# 17) What are the main steps involved in training a CNN from scratch?\n",
        "**Ans:** Training a Convolutional Neural Network (CNN) from scratch involves several systematic steps to ensure the model learns effectively from the data. Here's an overview of the main steps involved:\n",
        "\n",
        "1. **Data Collection and Preparation:**\n",
        "   - **Gather Data:** Collect a substantial and diverse dataset relevant to the task, such as labeled images for classification.\n",
        "   - **Data Preprocessing:** Ensure all images are uniform in format and size, and normalize pixel values to standardize the input data.\n",
        "   - **Data Augmentation:** Apply transformations like rotation, scaling, and flipping to artificially expand the dataset and improve model generalization.\n",
        "\n",
        "2. **Define the CNN Architecture:**\n",
        "   - **Layer Selection:** Design the network by choosing the number and types of layers, such as convolutional, pooling, and fully connected layers.\n",
        "   - **Hyperparameters:** Set hyperparameters including filter sizes, number of filters, stride lengths, and activation functions.\n",
        "\n",
        "3. **Initialize Weights:**\n",
        "   - Initialize the network's weights, often using methods like Xavier or He initialization to facilitate efficient training.\n",
        "\n",
        "4. **Forward Propagation:**\n",
        "   - Pass input data through the network to obtain output predictions.\n",
        "\n",
        "5. **Compute Loss:**\n",
        "   - Calculate the loss using an appropriate loss function (e.g., cross-entropy loss for classification) to measure the discrepancy between predictions and actual labels.\n",
        "\n",
        "6. **Backward Propagation (Backpropagation):**\n",
        "   - Compute gradients of the loss with respect to the network's weights to understand how changes in weights affect the loss.\n",
        "\n",
        "7. **Update Weights:**\n",
        "   - Adjust the weights using optimization algorithms like Stochastic Gradient Descent (SGD) or Adam, guided by the computed gradients.\n",
        "\n",
        "8. **Iterate Training Over Epochs:**\n",
        "   - Repeat the forward and backward propagation steps over multiple epochs, iterating through the entire training dataset to progressively minimize loss.\n",
        "\n",
        "9. **Validation:**\n",
        "   - Regularly evaluate the model's performance on a separate validation dataset to monitor for overfitting and adjust hyperparameters as needed.\n",
        "\n",
        "10. **Testing:**\n",
        "    - After training, assess the model's performance on a test dataset to estimate its effectiveness on unseen data.\n",
        "\n",
        "11. **Model Optimization and Regularization:**\n",
        "    - **Regularization Techniques:** Implement methods like dropout, weight decay, or early stopping to prevent overfitting and enhance model generalization.\n",
        "    - **Hyperparameter Tuning:** Adjust hyperparameters based on validation performance to optimize the model's accuracy and efficiency.\n"
      ],
      "metadata": {
        "id": "Pb0GO1Qtp0YA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "XhuPahv_hkqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Implement a basic convolution operation using a filter and a 5x5 image (matrix)."
      ],
      "metadata": {
        "id": "L9SzoT_vhm7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define a 5x5 image (matrix)\n",
        "image = np.array([\n",
        "    [1, 2, 3, 4, 5],\n",
        "    [5, 6, 7, 8, 9],\n",
        "    [9, 10, 11, 12, 13],\n",
        "    [13, 14, 15, 16, 17],\n",
        "    [17, 18, 19, 20, 21]\n",
        "])\n",
        "\n",
        "# Define a 3x3 filter (kernel)\n",
        "filter = np.array([\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1],\n",
        "    [1, 0, -1]\n",
        "])\n",
        "\n",
        "# Get dimensions\n",
        "image_height, image_width = image.shape\n",
        "filter_height, filter_width = filter.shape\n",
        "\n",
        "# Calculate the output dimensions\n",
        "output_height = image_height - filter_height + 1\n",
        "output_width = image_width - filter_width + 1\n",
        "\n",
        "# Initialize the output matrix\n",
        "output = np.zeros((output_height, output_width))\n",
        "\n",
        "# Perform the convolution operation\n",
        "for i in range(output_height):\n",
        "    for j in range(output_width):\n",
        "        # Extract the region of interest\n",
        "        region = image[i:i+filter_height, j:j+filter_width]\n",
        "        # Element-wise multiplication and sum\n",
        "        output[i, j] = np.sum(region * filter)\n",
        "\n",
        "print(\"Input Image:\")\n",
        "print(image)\n",
        "\n",
        "print(\"\\nFilter:\")\n",
        "print(filter)\n",
        "\n",
        "print(\"\\nConvolved Output:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "FZ4J_OaVhsGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Implement max pooling on a 4x4 feature map with a 2x2 window."
      ],
      "metadata": {
        "id": "op4eC2FwiGRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the 4x4 input feature map\n",
        "input_feature_map = np.array([[1, 3, 2, 4],\n",
        "                              [5, 6, 7, 8],\n",
        "                              [9, 10, 11, 12],\n",
        "                              [13, 14, 15, 16]])\n",
        "\n",
        "# Define the size of the pooling window and stride\n",
        "pool_size = 2\n",
        "stride = 2\n",
        "\n",
        "# Calculate the dimensions of the output feature map\n",
        "output_height = (input_feature_map.shape[0] - pool_size) // stride + 1\n",
        "output_width = (input_feature_map.shape[1] - pool_size) // stride + 1\n",
        "\n",
        "# Initialize the output feature map\n",
        "output_feature_map = np.zeros((output_height, output_width))\n",
        "\n",
        "# Apply max pooling\n",
        "for i in range(output_height):\n",
        "    for j in range(output_width):\n",
        "        # Define the current window\n",
        "        window = input_feature_map[i*stride:i*stride+pool_size, j*stride:j*stride+pool_size]\n",
        "        # Apply max pooling\n",
        "        output_feature_map[i, j] = np.max(window)\n",
        "\n",
        "print(output_feature_map)"
      ],
      "metadata": {
        "id": "11SK1D5TiL7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Implement the ReLU activation function on a feature map."
      ],
      "metadata": {
        "id": "-i9V5PvAkRkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the 4x4 feature map\n",
        "feature_map = np.array([[1, -2, 3, -4],\n",
        "                        [-5, 6, -7, 8],\n",
        "                        [9, -10, 11, -12],\n",
        "                        [-13, 14, -15, 16]])\n",
        "\n",
        "# Apply ReLU\n",
        "relu_feature_map = np.maximum(0, feature_map)\n",
        "\n",
        "print(relu_feature_map)\n"
      ],
      "metadata": {
        "id": "sTBdApDCkmV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Create a simple CNN model with one convolutional layer and a fully connected layer, using random data."
      ],
      "metadata": {
        "id": "g2S7vw6Fkzqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Generate random data\n",
        "input_shape = (100, 28, 28, 1)  # 100 samples, 28x28 pixels, 1 channel (grayscale)\n",
        "X_train = np.random.rand(*input_shape).astype(np.float32)\n",
        "y_train = np.random.randint(0, 10, size=(100,))\n",
        "\n",
        "# Build the CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "fgWXujhGlaf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Generate a synthetic dataset using random noise and train a simple CNN model on it."
      ],
      "metadata": {
        "id": "MFI89ZP5lbQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate random data\n",
        "num_samples = 100\n",
        "image_shape = (28, 28, 1)  # 28x28 pixels, 1 channel (grayscale)\n",
        "X_train = np.random.rand(num_samples, *image_shape).astype(np.float32)\n",
        "y_train = np.random.randint(0, 10, size=(num_samples,))\n",
        "\n",
        "# Visualize the first 5 images\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 15))\n",
        "for i in range(5):\n",
        "    axes[i].imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Build the CNN model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"Loss: {loss}\")\n",
        "print(f\"Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "WxXzHreylmEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Create a simple CNN using Keras with one convolution layer and a max-pooling layer."
      ],
      "metadata": {
        "id": "5PMLDUO6mBX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    # Convolutional layer with 32 filters, 3x3 kernel, ReLU activation, and input shape\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "\n",
        "    # Max-pooling layer with 2x2 pool size\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten the output to feed into the fully connected layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected (dense) layer with 64 units and ReLU activation\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Output layer with 10 units (for 10 classes) and softmax activation\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "juz7nyKimaW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Write a code to add a fully connected layer after the convolution and max-pooling layers in a CNN."
      ],
      "metadata": {
        "id": "Ep63TtIzm0QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    # Convolutional layer with 32 filters, 3x3 kernel, ReLU activation, and input shape\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "\n",
        "    # Max-pooling layer with 2x2 pool size\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten the output to feed into the fully connected layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected (dense) layer with 64 units and ReLU activation\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Output layer with 10 units (for 10 classes) and softmax activation\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "2ul7OT3Lm3rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Write a code to add  batch normalization to a simple CNN model."
      ],
      "metadata": {
        "id": "rTS3F1vCnFMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    # Convolutional layer with 32 filters, 3x3 kernel, ReLU activation, and input shape\n",
        "    Conv2D(32, (3, 3), input_shape=(28, 28, 1)),\n",
        "    BatchNormalization(),  # Apply Batch Normalization\n",
        "    tf.keras.layers.ReLU(),  # ReLU activation function\n",
        "\n",
        "    # Max-pooling layer with 2x2 pool size\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Flatten the output to feed into the fully connected layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected (dense) layer with 64 units and ReLU activation\n",
        "    Dense(64),\n",
        "    BatchNormalization(),  # Apply Batch Normalization\n",
        "    tf.keras.layers.ReLU(),  # ReLU activation function\n",
        "\n",
        "    # Output layer with 10 units (for 10 classes) and softmax activation\n",
        "    Dense(10, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "FRj-AYDDnMUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Write a code to add dropout regularization to a simple CNN model."
      ],
      "metadata": {
        "id": "kJxEr1lenZ1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    # Convolutional layer with 32 filters, 3x3 kernel, ReLU activation, and input shape\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "\n",
        "    # Max-pooling layer with 2x2 pool size\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "\n",
        "    # Dropout layer with a rate of 0.25 (25% of the units will be dropped)\n",
        "    Dropout(0.25),\n",
        "\n",
        "    # Flatten the output to feed into the fully connected layer\n",
        "    Flatten(),\n",
        "\n",
        "    # Fully connected (dense) layer with 64 units and ReLU activation\n",
        "    Dense(64, activation='relu'),\n",
        "\n",
        "    # Dropout layer with a rate of 0.5 (50% of the units will be dropped)\n",
        "    Dropout(0.5),\n",
        "\n",
        "    # Output layer with 10 units (for 10 classes) and softmax activation\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "OJ9V4yV3ngoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) Write a code to print the architecture of the VGG16 model in Keras."
      ],
      "metadata": {
        "id": "tNK2FQ5xn3As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "# Load the VGG16 model with pre-trained ImageNet weights\n",
        "model = VGG16(weights='imagenet')\n",
        "\n",
        "# Print the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GuMX8idhn6WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11) Write a code to plot the accuracy and loss graphs after training a CNN model."
      ],
      "metadata": {
        "id": "S7RfExhkoAnx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'model' is your CNN model and 'X_train', 'y_train' are your training data and labels\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "\n",
        "# Create a figure with two subplots: one for accuracy and one for loss\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "ax1.set_title('Training and Validation Accuracy')\n",
        "ax1.set_xlabel('Epochs')\n",
        "ax1.set_ylabel('Accuracy')\n",
        "ax1.legend()\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plot training and validation loss\n",
        "ax2.plot(history.history['loss'], label='Training Loss')\n",
        "ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
        "ax2.set_title('Training and Validation Loss')\n",
        "ax2.set_xlabel('Epochs')\n",
        "ax2.set_ylabel('Loss')\n",
        "ax2.legend()\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Display the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GIqm1WXMoOOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12) Write a code to print the architecture of the ResNet50 model in Keras."
      ],
      "metadata": {
        "id": "ozNmYhhCow65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the ResNet50 model with pre-trained ImageNet weights\n",
        "model = ResNet50(weights='imagenet')\n",
        "\n",
        "# Print the model architecture\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "FCvhN04Tov3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13) Write a code to train a basic CNN model and print the training loss and accuracy after each epoch?"
      ],
      "metadata": {
        "id": "YqpokFIxo3z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Assuming X_train and y_train are your training data and labels\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "for epoch in range(10):\n",
        "    # Access the loss and accuracy for the current epoch\n",
        "    loss = history.history['loss'][epoch]\n",
        "    accuracy = history.history['accuracy'][epoch]\n",
        "    val_loss = history.history['val_loss'][epoch]\n",
        "    val_accuracy = history.history['val_accuracy'][epoch]\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Epoch {epoch+1}/{10}\")\n",
        "    print(f\"Training Loss: {loss:.4f} - Training Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f} - Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "Q45MQpDzo8D7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}