{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) What is the primary goal of Natural Language Processing (NLP)?\n",
        "\n",
        "**Ans:** The primary goal of **Natural Language Processing (NLP)** is to enable machines to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
        "\n",
        "This involves:\n",
        "\n",
        "1. **Understanding Text and Speech**: Analyzing the structure, meaning, and intent behind human language.\n",
        "2. **Human-Machine Communication**: Allowing humans to interact with computers using natural language (e.g., chatbots, virtual assistants).\n",
        "3. **Automating Language Tasks**: Performing tasks like translation, summarization, sentiment analysis, and text classification.\n",
        "4. **Extracting Insights**: Deriving valuable information from large amounts of unstructured text data.\n",
        "\n",
        "#2) What does \"tokenization\" refer to in text processing?\n",
        "\n",
        "**Ans:** In text processing, tokenization refers to dividing a text into smaller units called tokens. These tokens are typically words, phrases, or even individual characters, depending on the application. Tokenization is a fundamental step in Natural Language Processing (NLP) as it transforms unstructured text into a structured format that machines can more easily analyze and process.\n",
        "\n",
        "#3) What is the difference between lemmatization and stemming?\n",
        "**Ans:** In text processing, stemming and lemmatization are techniques used to reduce words to their base or root forms, aiding in tasks like text normalization and information retrieval.\n",
        "\n",
        "**Stemming:**\n",
        "\n",
        "- Involves removing prefixes or suffixes from words to obtain a common root form, often using simple heuristic methods.\n",
        "\n",
        "- The resulting stem may not be a valid word; for example, \"caring\" might be reduced to \"car\".\n",
        "\n",
        "- Generally faster due to its rule-based approach but can lead to less accurate results.\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        "- Reduces words to their base or dictionary form, known as the lemma, by considering the word's context and part of speech.\n",
        "\n",
        "- The output is a valid word; for instance, \"caring\" becomes \"care\".\n",
        "\n",
        "- More accurate as it accounts for the word's meaning and grammatical role but is computationally more intensive."
      ],
      "metadata": {
        "id": "wSagfqO7KN7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) What is the role of regular expressions (regex) in text processing?\n",
        "**Ans:** Regular expressions, commonly known as **regex** or **regexp**, are sequences of characters that define search patterns. They play a crucial role in text processing by enabling efficient pattern matching and manipulation within strings.\n",
        "\n",
        "**Key roles of regular expressions in text processing include:**\n",
        "\n",
        "- **Pattern Matching**: Identifying specific sequences within text, such as email addresses, phone numbers, or dates. For example, a regex can be crafted to match all email addresses in a document.\n",
        "\n",
        "- **Search and Replace**: Modifying text by finding patterns and replacing them. This is useful for tasks like correcting formatting issues or standardizing data entries.\n",
        "\n",
        "- **Input Validation**: Ensuring that user inputs adhere to expected formats, such as validating that a string conforms to the pattern of a valid email address or phone number.\n",
        "\n",
        "- **Text Extraction**: Pulling out specific data from larger text bodies, such as extracting all URLs from a webpage or all capitalized words from a document.\n",
        "\n",
        "- **Data Cleaning**: Removing unwanted characters, extra spaces, or formatting inconsistencies to prepare text for further analysis.\n",
        "\n",
        "# 5) What is Word2Vec and how does it represent words in a vector space?\n",
        "**Ans:** **Word2Vec** is a technique in natural language processing (NLP) that transforms words into continuous vector representations, known as word embeddings. Developed by Tomáš Mikolov and colleagues at Google in 2013, Word2Vec captures semantic and syntactic relationships between words by analyzing large text corpora.\n",
        "\n",
        "**How Word2Vec Represents Words in Vector Space:**\n",
        "\n",
        "1. **Training Process:**\n",
        "   - Word2Vec utilizes a shallow, two-layer neural network to process a large corpus of text. It employs one of two model architectures: Continuous Bag of Words (CBOW) or Continuous Skip-gram. CBOW predicts a target word based on its surrounding context words, while Skip-gram predicts surrounding context words given a target word.\n",
        "\n",
        "2. **Vector Representation:**\n",
        "   - Through training, each word in the vocabulary is assigned a vector in a continuous vector space, typically of several hundred dimensions. These vectors are positioned such that words sharing similar contexts are located near each other, effectively capturing semantic similarities.\n",
        "\n",
        "3. **Semantic Relationships:**\n",
        "   - The resulting word embeddings encode semantic relationships, allowing for operations like vector arithmetic to uncover word associations. For example, the vector operation **vec(\"King\") - vec(\"Man\") + vec(\"Woman\")** results in a vector close to **vec(\"Queen\")**, demonstrating how Word2Vec captures gender relationships.\n",
        "\n",
        "4. **Dimensionality and Context:**\n",
        "   - The dimensionality of the vectors and the size of the context window are crucial hyperparameters. Typically, vector dimensions range between 100 and 1,000. The context window size determines how many words before and after a given word are considered, influencing the embedding's ability to capture broader or narrower contextual relationships.\n",
        "\n",
        "# 6) How does frequency distribution help in text analysis?\n",
        "**Ans:** In text analysis, **frequency distribution** refers to the tabulation of how often each word or token appears within a given text or corpus. This method is fundamental for several reasons:\n",
        "\n",
        "1. **Identifying Key Themes and Topics**: By pinpointing the most frequently occurring words, analysts can discern the central themes or subjects of a text. For instance, in a collection of news articles, a high frequency of words like \"election,\" \"candidate,\" and \"vote\" suggests a focus on political events.\n",
        "\n",
        "2. **Facilitating Data Cleaning**: Frequency analysis aids in detecting and removing common stop words (e.g., \"the,\" \"and,\" \"is\") that may not contribute meaningful information to the analysis. Additionally, it helps identify and correct typographical errors or inconsistencies by highlighting unusual word occurrences.\n",
        "\n",
        "3. **Supporting Cryptanalysis**: In the context of deciphering encoded messages, frequency analysis can be instrumental. By examining the frequency of letters or groups of letters in a cipher text, one can make educated guesses about the substitutions used, thereby aiding in decryption efforts.\n",
        "\n",
        "4. **Enhancing Information Retrieval**: Understanding word frequency distributions enables the development of more effective search algorithms and indexing systems. By assigning appropriate weights to terms based on their frequency, search engines can improve the relevance of retrieved documents.\n",
        "\n",
        "5. **Visualizing Data**: Frequency distributions can be graphically represented to provide intuitive insights into the text. Tools and methods are available to visualize these distributions, making it easier to interpret and communicate findings.\n",
        "\n",
        "# 7) Why is text normalization important in NLP?\n",
        "**Ans:** **Text normalization** is a crucial preprocessing step in Natural Language Processing (NLP) that involves converting text into a standard, consistent format. This process enhances the performance and accuracy of NLP models in several ways:\n",
        "\n",
        "1. **Reducing Complexity**: Natural language is inherently diverse, with variations in spelling, capitalization, punctuation, and formatting. Text normalization mitigates this complexity by standardizing these elements, making it easier for models to process and analyze the data.\n",
        "\n",
        "2. **Improving Model Efficiency**: By reducing the number of unique tokens through normalization, models can operate more efficiently. This reduction leads to decreased computational requirements and faster processing times.\n",
        "\n",
        "3. **Enhancing Data Quality**: Normalization addresses inconsistencies and errors in the text, such as typographical mistakes or irregular abbreviations, resulting in cleaner and more reliable data for analysis.\n",
        "\n",
        "4. **Facilitating Better Feature Extraction**: Standardized text allows for more accurate extraction of linguistic features, which are essential for tasks like sentiment analysis, machine translation, and information retrieval.\n",
        "\n",
        "5. **Ensuring Consistency Across Datasets**: When dealing with multiple data sources, normalization ensures that text is uniformly formatted, enabling seamless integration and comparison across datasets.\n",
        "\n",
        "# 8) What is the difference between sentence tokenization and word tokenization?\n",
        "**Ans:**In Natural Language Processing (NLP), **tokenization** is the process of dividing text into smaller units called tokens. The two primary forms of tokenization are **sentence tokenization** and **word tokenization**.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "- **Scope**: Sentence tokenization deals with larger text units by identifying sentence boundaries, while word tokenization focuses on dividing sentences into individual words.\n",
        "\n",
        "- **Applications**: Sentence tokenization is vital for understanding the overall structure and meaning of a text, aiding in tasks that require sentence-level analysis. In contrast, word tokenization is essential for tasks that involve word-level analysis, such as building vocabularies for language models.\n",
        "\n",
        "- **Complexity**: Sentence tokenization can be more complex due to the need to accurately identify sentence boundaries, which may be obscured by abbreviations, punctuation, and other language nuances. Word tokenization, while generally more straightforward, must handle challenges like contractions and hyphenated words.\n",
        "\n",
        "# 9) What are co-occurrence vectors in NLP?\n",
        "**Ans:** Co-occurrence vectors in NLP are mathematical representations of words based on their frequency of appearing together (co-occurring) in a given context within a text corpus. These vectors are derived from co-occurrence matrices, where each word in the vocabulary is represented as a vector that encodes its relationship with all other words.\n",
        "# 10) What is the significance of lemmatization in improving NLP tasks?\n",
        "**Ans:**\n",
        "\n",
        "**Significance of Lemmatization in NLP:**\n",
        "\n",
        "1. **Enhancing Text Consistency**: By converting various inflected forms of a word to a common base form, lemmatization ensures that words like \"running,\" \"ran,\" and \"runs\" are all recognized as \"run.\" This standardization is crucial for consistent text analysis.\n",
        "\n",
        "2. **Improving Information Retrieval**: Lemmatization aids in matching user queries with relevant documents by aligning different word forms to a single lemma, thereby enhancing search accuracy.\n",
        "\n",
        "3. **Reducing Feature Space Dimensionality**: By consolidating word variants into a single representation, lemmatization decreases the number of unique tokens in a dataset. This reduction simplifies computational models and can lead to more efficient processing.\n",
        "\n",
        "4. **Enhancing Model Performance**: Standardizing words to their lemmas allows NLP models to better recognize patterns and relationships within the data, potentially improving the accuracy of tasks such as sentiment analysis and machine translation.\n",
        "\n",
        "5. **Facilitating Semantic Understanding**: Lemmatization helps in understanding the context and meaning of words by considering their part of speech and intended meaning, which is essential for tasks like word sense disambiguation.\n",
        "\n",
        "# 11) What is the primary use of word embeddings in NLP?\n",
        "**Ans:**\n",
        "**Primary Uses of Word Embeddings in NLP:**\n",
        "\n",
        "1. **Capturing Semantic Relationships**: Word embeddings position words with similar meanings close to each other in the vector space, allowing models to recognize and leverage semantic similarities.\n",
        "\n",
        "2. **Improving Model Performance**: By providing dense and informative representations of words, embeddings enhance the performance of various NLP tasks, including text classification, named entity recognition, and machine translation.\n",
        "\n",
        "3. **Reducing Dimensionality**: Unlike traditional one-hot encoding, which results in high-dimensional sparse vectors, word embeddings offer a lower-dimensional representation, reducing computational complexity and memory usage.\n",
        "\n",
        "4. **Facilitating Transfer Learning**: Pre-trained word embeddings can be utilized across different NLP tasks and domains, enabling models to benefit from prior knowledge and reducing the need for extensive training data.\n",
        "\n",
        "5. **Enhancing Contextual Understanding**: Word embeddings help models grasp the context in which words appear, improving tasks like sentiment analysis and information retrieval.\n",
        "\n",
        "# 12) What is an annotator in NLP?\n",
        "**Ans:** In Natural Language Processing (NLP), an annotator refers to a tool or individual responsible for labeling and enriching text data with additional information, known as annotations. These annotations provide context and structure to raw text, enabling machines to better understand and process human language.\n",
        "# 13) What are the key steps in text processing before applying machine learning models?\n",
        "**Ans:** Key steps in text preprocessing include:\n",
        "\n",
        "1. **Text Cleaning**: This involves removing or correcting noisy elements such as punctuation, numbers, special characters, and correcting misspellings to ensure uniformity.\n",
        "\n",
        "2. **Lowercasing**: Converting all text to lowercase to maintain consistency, as \"Apple\" and \"apple\" would be treated the same.\n",
        "\n",
        "3. **Tokenization**: Splitting text into individual units like words or phrases, known as tokens, which serve as the basic building blocks for further analysis.\n",
        "\n",
        "4. **Stopword Removal**: Eliminating common words (e.g., \"the,\" \"is,\" \"and\") that may not carry significant meaning and could introduce noise into the model.\n",
        "\n",
        "5. **Stemming and Lemmatization**: Reducing words to their root or base form to treat different forms of a word as a single entity, aiding in uniformity.\n",
        "\n",
        "6. **Removing Punctuation and Special Characters**: Eliminating unnecessary punctuation marks and special characters that do not contribute to the analysis.\n",
        "\n",
        "7. **Handling Contractions**: Expanding contractions (e.g., \"don't\" to \"do not\") to ensure consistency in text representation.\n",
        "\n",
        "8. **Removing Numbers**: Depending on the context, numbers may be removed if they are not relevant to the analysis.\n",
        "\n",
        "9. **Text Normalization**: Standardizing text by correcting misspellings and ensuring consistent formatting.\n",
        "\n",
        "10. **Feature Extraction**: Converting text data into numerical representations suitable for machine learning algorithms, such as Bag of Words, TF-IDF, or word embeddings.\n",
        "\n",
        "# 14) What is the history of NLP and how has it evolved?\n",
        "**Ans:** Natural Language Processing (NLP) has evolved significantly since its inception, transitioning from rule-based systems to advanced machine learning models that enable machines to comprehend and generate human language with increasing sophistication.\n",
        "\n",
        "**Early Beginnings (1950s-1960s):**\n",
        "\n",
        "The origins of NLP trace back to the 1950s, with initial efforts focused on machine translation, particularly between Russian and English during the Cold War era. In 1957, Noam Chomsky's publication of *Syntactic Structures* introduced transformational grammar, profoundly influencing computational linguistics by emphasizing the importance of syntactic structures in understanding language.\n",
        "\n",
        "**Rule-Based Systems (1960s-1970s):**\n",
        "\n",
        "During the 1960s and 1970s, NLP research predominantly utilized rule-based systems, where linguists and computer scientists developed handcrafted grammatical rules and dictionaries to process language. Systems like SHRDLU demonstrated the potential of these approaches by understanding and executing complex tasks based on natural language commands.\n",
        "\n",
        "**Statistical Methods and Machine Learning (1980s-1990s):**\n",
        "\n",
        "The late 1980s marked a paradigm shift with the introduction of statistical methods and machine learning algorithms in NLP. This transition enabled the development of models that could learn from large datasets, improving tasks such as speech recognition and part-of-speech tagging.\n",
        "\n",
        "**Advancements in the 21st Century:**\n",
        "\n",
        "The 21st century has witnessed remarkable progress in NLP, driven by the advent of deep learning and neural network architectures. In 2017, the introduction of the Transformer architecture revolutionized the field by enabling models to process text non-sequentially, leading to significant improvements in machine translation, text summarization, and other language-related tasks.\n",
        "\n",
        "**Current Trends and Future Directions:**\n",
        "\n",
        "Today, NLP continues to advance with the development of large-scale language models capable of understanding and generating human-like text. Ongoing research focuses on enhancing contextual understanding, reducing biases, and improving the efficiency and scalability of NLP systems.\n",
        "\n",
        "\n",
        "# 15) Why is sentence processing important in NLP?\n",
        "**Ans:** Sentence processing importance in NLP is underscored by several key factors:\n",
        "\n",
        "1. **Understanding Context and Meaning**: Sentences are the primary units through which ideas and information are conveyed. Processing sentences enables NLP systems to grasp the context and semantics, facilitating accurate interpretation of the intended message.\n",
        "\n",
        "2. **Syntactic and Semantic Analysis**: Effective sentence processing involves parsing grammatical structures and understanding relationships between words, which is essential for tasks like machine translation, sentiment analysis, and information extraction.\n",
        "\n",
        "3. **Improving Human-Computer Interaction**: By accurately processing sentences, NLP systems can engage in more natural and meaningful interactions with users, enhancing the overall user experience.\n",
        "\n",
        "4. **Enabling Advanced Applications**: Many sophisticated NLP applications, such as question answering systems and chatbots, rely on robust sentence processing to function effectively. Understanding sentence structure and meaning allows these systems to generate appropriate and contextually relevant responses.\n",
        "\n",
        "# 16) How do word embeddings improve the understanding of language semantics in NLP?\n",
        "**Ans:** Word embeddings are a foundational technique in Natural Language Processing (NLP) that enhance the understanding of language semantics by representing words as continuous, dense vectors in a high-dimensional space. This approach offers several advantages:\n",
        "\n",
        "1. **Capturing Semantic Relationships**: Word embeddings position words with similar meanings closer together in the vector space, effectively capturing semantic relationships. For instance, the vectors for \"king\" and \"queen\" would be proximate, reflecting their related meanings.\n",
        "\n",
        "2. **Contextual Understanding**: By analyzing the contexts in which words appear, embeddings encapsulate nuanced meanings and associations, enabling models to distinguish between different senses of a word based on its usage.\n",
        "\n",
        "3. **Dimensionality Reduction**: Transforming words into vector representations reduces the complexity of language data, making it more manageable for machine learning algorithms to process and analyze.\n",
        "\n",
        "4. **Enhancing Model Performance**: Incorporating word embeddings into NLP models has been shown to improve performance across various tasks, including sentiment analysis, machine translation, and named entity recognition, by providing a richer semantic understanding of the text.\n",
        "\n",
        "# 17) How does the frequency distribution of words help in text classification?\n",
        "**Ans:** Understanding the frequency distribution of words is fundamental in text classification tasks within Natural Language Processing (NLP). Here's how it contributes:\n",
        "\n",
        "1. **Feature Representation**: Word frequency serves as a primary feature in models like Bag of Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF). In BoW, each document is represented by a vector indicating the frequency of words, enabling algorithms to process textual data numerically.\n",
        "\n",
        "2. **Identifying Distinctive Terms**: Analyzing word frequency helps identify terms that are characteristic of specific categories. Words that appear frequently in one category but not in others can serve as strong indicators for classification.\n",
        "\n",
        "3. **Dimensionality Reduction**: By examining word frequency distributions, it's possible to filter out common words (stopwords) that don't contribute to classification, thereby reducing dimensionality and improving model performance.\n",
        "\n",
        "4. **Enhancing Model Accuracy**: Incorporating word frequency information into feature weighting schemes, such as TF-IDF, enhances the model's ability to distinguish between important and less important words, leading to improved classification accuracy.\n",
        "\n",
        "# 18)  What are the advantages of using regex in text cleaning?\n",
        "**Ans:** Regular expressions (regex) are a powerful tool in text cleaning, offering several advantages:\n",
        "\n",
        "1. **Flexible Pattern Matching**: Regex allows for the identification and manipulation of complex text patterns, enabling tasks such as removing unwanted characters, standardizing formats, and extracting specific information.\n",
        "\n",
        "2. **Conciseness**: Regex provides a concise way to perform complex text manipulations, reducing the need for lengthy code.\n",
        "\n",
        "3. **Efficiency**: Regex operations are optimized for speed, allowing for quick processing of large datasets.\n",
        "\n",
        "4. **Language Agnosticism**: Regex syntax is consistent across many programming languages, making it a versatile tool for text processing tasks.\n",
        "\n",
        "\n",
        "# 19) What is the difference between word2vec and doc2vec?\n",
        "**Ans:** Word2Vec and Doc2Vec are both algorithms designed to generate vector representations of textual data, but they operate at different levels and serve distinct purposes:\n",
        "\n",
        "**Word2Vec**:\n",
        "\n",
        "- **Purpose**: Generates vector representations for individual words.\n",
        "\n",
        "- **Functionality**: Models semantic relationships between words by placing similar words closer together in the vector space.\n",
        "\n",
        "- **Use Cases**: Ideal for tasks requiring word-level analysis, such as identifying synonyms, analogies, or word clustering.\n",
        "\n",
        "**Doc2Vec**:\n",
        "\n",
        "- **Purpose**: Extends the Word2Vec approach to generate vector representations for larger textual units like sentences, paragraphs, or entire documents.\n",
        "\n",
        "- **Functionality**: Captures the semantic essence of a document by considering the context and order of words, enabling the representation of the document's overall meaning.\n",
        "\n",
        "- **Use Cases**: Suitable for document-level tasks such as document classification, clustering, or retrieval.\n",
        "\n",
        "\n",
        "# 20) Why is understanding text normalization important in NLP?\n",
        "**Ans:** Text normalization is a crucial process in Natural Language Processing (NLP) that involves converting diverse text formats into a consistent, standard form. This standardization is essential for several reasons:\n",
        "\n",
        "1. **Enhancing Data Consistency**: Raw text data often contains variations such as different tenses, superlatives, abbreviations, and colloquialisms. Normalization reduces these inconsistencies, ensuring that words like \"running\" and \"ran\" are treated as instances of the base form \"run,\" thereby simplifying the data.\n",
        "\n",
        "2. **Improving Model Efficiency**: By standardizing text, normalization reduces the complexity of the data, making it more manageable for machine learning models. This simplification leads to more efficient training and improved performance in NLP tasks.\n",
        "\n",
        "3. **Reducing Vocabulary Size**: Normalization techniques like stemming and lemmatization decrease the number of unique tokens in the dataset. This reduction helps in managing the curse of dimensionality and enhances the generalization capabilities of NLP models.\n",
        "\n",
        "4. **Facilitating Accurate Text Analysis**: Standardized text allows for more precise analysis in tasks such as sentiment analysis, information retrieval, and machine translation. Without normalization, variations in text can lead to misinterpretations and decreased accuracy.\n",
        "\n",
        "5. **Handling Informal and Noisy Text**: In real-world applications, especially with data from social media and user-generated content, text often includes slang, typos, and non-standard expressions. Normalization processes these irregularities, converting them into a form that NLP models can effectively analyze.\n",
        "\n",
        "# 21) How does word count help in text analysis?\n",
        "**Ans:** Word count analysis is a fundamental technique in text analysis that offers several benefits:\n",
        "\n",
        "1. **Assessing Text Length and Structure**: Word count provides a quantitative measure of text length, aiding in evaluating the structure and complexity of documents. This is particularly useful for writers, editors, and researchers to ensure adherence to length requirements and to assess readability.\n",
        "\n",
        "2. **Facilitating Sentiment Analysis**: By analyzing the frequency of specific words, especially those with positive or negative connotations, word count can help predict the overall sentiment expressed in a text. For instance, a higher frequency of positive words may indicate a favorable sentiment.\n",
        "\n",
        "3. **Supporting Text Classification**: Word count analysis assists in categorizing texts by identifying distinctive word usage patterns across different categories. This method is foundational in various text classification tasks, such as spam detection and topic categorization.\n",
        "\n",
        "4. **Enhancing Data Analysis**: In data analysis, word count serves as a valuable metric for understanding text data. For instance, analyzing the word count of customer reviews or social media posts can provide insights into user engagement and content effectiveness.\n",
        "\n",
        "# 22) How does lemmatization help in NLP tasks like search engines and chatbots?\n",
        "**Ans:** Lemmatization is essential for enhancing the performance of applications like search engines and chatbots in several ways:\n",
        "\n",
        "1. **Improved Search Accuracy**: By reducing words to their base forms, lemmatization ensures that search engines can match queries with all relevant variations of a term. For instance, a search for \"running\" will also retrieve results containing \"run\" or \"ran,\" thereby broadening the search scope and improving accuracy.\n",
        "\n",
        "2. **Enhanced Chatbot Understanding**: In chatbot interactions, lemmatization helps in recognizing and interpreting user inputs more effectively by understanding the context and meaning of words in their base forms. This leads to more accurate responses and a better user experience.\n",
        "\n",
        "3. **Efficient Information Retrieval**: Lemmatization aids in grouping different inflected forms of a word, which is particularly beneficial for information retrieval systems. This grouping allows for more effective and accurate tools such as chatbots and search engine queries.\n",
        "\n",
        "4. **Reduced Computational Complexity**: By standardizing words to their lemmas, lemmatization reduces the dimensionality of text data. This simplification leads to more efficient processing and analysis, enhancing the performance of NLP models used in search engines and chatbots.\n",
        "\n",
        "# 23) What is the purpose of using Doc2Vec in text processing?\n",
        "**Ans:** Doc2Vec is a technique in Natural Language Processing (NLP) that generates vector representations for entire documents, capturing their semantic content in a fixed-length numerical format. This method extends the principles of Word2Vec, which creates embeddings for individual words, to larger text structures such as sentences, paragraphs, or full documents.\n",
        "\n",
        "**Purpose of Using Doc2Vec in Text Processing:**\n",
        "\n",
        "1. **Semantic Representation**: Doc2Vec encodes the semantic meaning and context of a document into a vector, facilitating the understanding of the document's content by machine learning models.\n",
        "\n",
        "2. **Document Classification**: By representing documents as vectors, Doc2Vec enables efficient classification tasks, allowing models to group similar documents together based on their content.\n",
        "\n",
        "3. **Information Retrieval**: In search engines and recommendation systems, Doc2Vec aids in retrieving documents that are semantically similar to a user's query, enhancing the relevance of search results.\n",
        "\n",
        "4. **Clustering and Topic Modeling**: Doc2Vec facilitates the grouping of documents into clusters or topics based on content similarity, aiding in the organization and summarization of large text corpora.\n",
        "\n",
        "5. **Handling Unseen Data**: Doc2Vec can infer vectors for new, unseen documents by analyzing their content in relation to the existing corpus, making it adaptable to dynamic datasets.\n",
        "\n",
        "# 24) What is the importance of sentence processing in NLP?\n",
        "**Ans:** Sentence processing is a fundamental aspect of Natural Language Processing (NLP) that involves analyzing and understanding the structure and meaning of sentences. Its importance in NLP is multifaceted:\n",
        "\n",
        "1. **Syntactic Analysis**: Sentence processing enables the identification of grammatical structures, such as parts of speech and syntactic dependencies, which are essential for understanding the relationships between words. This analysis aids in parsing sentences to comprehend their hierarchical structure.\n",
        "\n",
        "2. **Semantic Understanding**: By analyzing sentences, NLP systems can interpret the meaning conveyed, facilitating tasks like sentiment analysis, information extraction, and machine translation. Understanding sentence semantics is crucial for accurately capturing the intent behind the text.\n",
        "\n",
        "3. **Contextual Interpretation**: Sentences provide context that helps in disambiguating word meanings and resolving references, enhancing the accuracy of language models in tasks such as question answering and dialogue systems. This contextual understanding is vital for generating coherent and relevant responses.\n",
        "\n",
        "4. **Information Extraction**: Processing sentences allows for the extraction of key information, such as entities and relationships, which is essential for building knowledge graphs and supporting search engines. Accurate sentence processing ensures that relevant information is correctly identified and utilized.\n",
        "\n",
        "5. **Improving NLP Applications**: Effective sentence processing enhances the performance of various NLP applications, including text summarization, sentiment analysis, and machine translation, by ensuring that the nuances of sentence structure and meaning are accurately captured. This leads to more reliable and user-friendly language processing tools.\n",
        "\n",
        "# 25) What is text normalization, and what are the common techniques used in it?\n",
        "**Ans:** Text normalization is a crucial preprocessing step in Natural Language Processing (NLP) that involves transforming text into a standard, consistent format. This process enhances the performance of NLP models by reducing variability and ensuring uniformity in textual data.\n",
        "\n",
        "**Common Techniques in Text Normalization:**\n",
        "\n",
        "1. **Lowercasing**: Converting all characters in the text to lowercase to ensure uniformity, as \"Apple\" and \"apple\" should be treated identically.\n",
        "\n",
        "2. **Removing Punctuation and Special Characters**: Eliminating punctuation marks and special symbols that may not contribute to the semantic meaning, thereby simplifying the text.\n",
        "\n",
        "3. **Tokenization**: Splitting text into individual words or tokens, facilitating easier analysis and processing.\n",
        "\n",
        "4. **Stemming**: Reducing words to their root forms by removing suffixes or prefixes, aiding in treating words like \"running\" and \"runner\" as the base form \"run.\"\n",
        "\n",
        "5. **Lemmatization**: Converting words to their base or dictionary form (lemma) by considering the context and morphological analysis, ensuring that \"better\" maps to \"good.\"\n",
        "\n",
        "6. **Removing Stop Words**: Eliminating common words such as \"and,\" \"the,\" and \"is\" that may not carry significant meaning in certain analyses, thereby focusing on the more informative parts of the text.\n",
        "\n",
        "7. **Handling Contractions**: Expanding contractions like \"don't\" to \"do not\" to maintain consistency and clarity in the text.\n",
        "\n",
        "8. **Standardizing Abbreviations and Acronyms**: Converting abbreviations to their full forms or ensuring consistent representation to avoid ambiguity.\n",
        "\n",
        "9. **Removing Extra Whitespaces**: Eliminating unnecessary spaces, tabs, or newline characters to ensure clean and consistent text formatting.\n",
        "\n",
        "# 26) Why is word tokenization important in NLP?\n",
        "**Ans:** Word tokenization is a fundamental preprocessing step in Natural Language Processing (NLP) that involves dividing text into individual words or tokens. This process is crucial for several reasons:\n",
        "\n",
        "1. **Facilitating Text Analysis**: By breaking down text into manageable units, tokenization enables the analysis of word frequency, context, and relationships, which are essential for understanding and interpreting language.\n",
        "\n",
        "2. **Enabling Feature Extraction**: Tokenization allows for the extraction of features such as n-grams, parts of speech, and named entities, which are vital for various NLP tasks like text classification and sentiment analysis.\n",
        "\n",
        "3. **Supporting Language Modeling**: In language models, tokenization provides the basic units for predicting the next word in a sequence, thereby aiding in tasks like text generation and machine translation.\n",
        "\n",
        "4. **Handling Ambiguity**: Tokenization helps disambiguate text by splitting it into individual tokens, which can then be analyzed in the context of their surrounding tokens. This context-aware approach provides a more nuanced understanding of the text and improves the accuracy of subsequent NLP tasks.\n",
        "\n",
        "# 27) How does sentence tokenization differ from word tokenization in NLP?\n",
        "**Ans:**\n",
        "1. **Granularity**: Sentence tokenization deals with larger text units (sentences), while word tokenization focuses on smaller units (words).\n",
        "\n",
        "2. **Delimiters**: Sentence tokenization primarily relies on punctuation marks that denote sentence endings, whereas word tokenization uses spaces and intra-sentence punctuation to identify word boundaries.\n",
        "\n",
        "3. **Applications**: Sentence tokenization is crucial for tasks that analyze or generate text at the sentence level, such as summarization or translation. In contrast, word tokenization is fundamental for tasks that require word-level analysis, like part-of-speech tagging or named entity recognition.\n",
        "\n",
        "# 28) What is the primary purpose of text processing in NLP?\n",
        "**Ans:** Text processing is a fundamental step in Natural Language Processing (NLP) that involves preparing and cleaning raw text data to make it suitable for analysis and modeling. The primary purpose of text processing is to transform unstructured text into a structured format that computational models can effectively interpret and analyze.\n",
        "\n",
        "# 29) What are the key challenges in NLP?\n",
        "**Ans:** Natural Language Processing (NLP) faces several key challenges that impact the development and effectiveness of language models and applications. Understanding these challenges is crucial for advancing NLP technologies.\n",
        "\n",
        "**1. Ambiguity and Polysemy**: Words and sentences often have multiple meanings depending on context. Disambiguating these meanings requires models to understand context deeply.\n",
        "\n",
        "**2. Data Sparsity and Quality**: High-quality, annotated datasets are essential for training effective NLP models. Limited or noisy data can hinder model performance.\n",
        "\n",
        "**3. Contextual Understanding**: Capturing the context in which words and phrases are used is vital for accurate interpretation. Models must comprehend nuances and implied meanings.\n",
        "\n",
        "**4. Multilingualism and Language Variations**: NLP systems must handle multiple languages and dialects, each with unique structures and idioms. This diversity complicates model training and application.\n",
        "\n",
        "**5. Ethical and Bias Concerns**: NLP models can inadvertently learn and perpetuate biases present in training data, leading to unfair or discriminatory outcomes. Addressing these biases is crucial for ethical AI deployment.\n",
        "\n",
        "**6. Scalability and Performance**: Processing large volumes of text data efficiently requires scalable models that maintain high performance across diverse tasks.\n",
        "\n",
        "**7. Robustness and Uncertainty**: NLP models must be robust to noisy or adversarial inputs and capable of quantifying uncertainty in their predictions to ensure reliability.\n"
      ],
      "metadata": {
        "id": "coJXd3BCS9vW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 30) How do co-occurrence vectors represent relationships between words?\n",
        "**Ans:** Co-occurrence vectors are instrumental in representing relationships between words by capturing the frequency with which words appear together within a specified context. This method is grounded in the linguistic principle that words frequently appearing together often share semantic or syntactic relationships.\n",
        "\n",
        "**Representation of Word Relationships:**\n",
        "\n",
        "- **Semantic Similarity**: Words with similar meanings tend to have comparable co-occurrence patterns. For instance, \"doctor\" and \"physician\" often appear in similar contexts, leading to co-occurrence vectors that are close in the vector space, indicating their semantic similarity.\n",
        "\n",
        "- **Syntactic Relationships**: Co-occurrence vectors can also capture syntactic relationships. Words that function similarly in sentences, such as verbs or adjectives, may exhibit similar co-occurrence patterns, reflecting their syntactic roles.\n",
        "\n",
        "- **Contextual Associations**: Words frequently appearing together, like \"coffee\" and \"cup,\" develop co-occurrence vectors that reflect their contextual association, even if they are not synonyms.\n",
        "\n",
        "# 31) What is the role of frequency distribution in text analysis?\n",
        "**Ans:** Frequency distribution is a fundamental tool in text analysis that involves counting how often each word or token appears within a given text or corpus. This analysis serves several key purposes:\n",
        "\n",
        "1. **Identifying Common Themes and Keywords**: By determining the most frequently occurring words, analysts can infer the central topics or themes of the text. This is particularly useful in summarizing content and extracting keywords.\n",
        "\n",
        "2. **Understanding Language Patterns**: Frequency analysis helps in understanding the structure and usage patterns of a language within the text, such as common phrases or collocations.\n",
        "\n",
        "3. **Supporting Cryptanalysis**: In cryptography, frequency analysis examines the frequency of letters or groups of letters in ciphertexts to break substitution ciphers. This method exploits the predictable frequency patterns of letters in a given language.\n",
        "\n",
        "4. **Facilitating Data Visualization**: Visual tools like frequency distribution plots can graphically represent the most common tokens in a text collection, aiding in the quick identification of prominent words and patterns.\n",
        "\n",
        "5. **Assisting in Feature Selection for Modeling**: In machine learning, understanding word frequency distributions can inform the selection of features for text classification models, helping to identify which words may be most predictive of certain outcomes.\n",
        "\n",
        "6. **Analyzing Word Statistics**: Frequency distribution analysis can reveal statistical properties of word usage, such as adherence to Zipf's law, which states that the frequency of any word is inversely proportional to its rank in the frequency table.\n",
        "\n",
        "# 32) What is the impact of word embeddings on NLP tasks?\n",
        "**Ans:** Word embeddings have significantly advanced Natural Language Processing (NLP) by providing dense vector representations of words, capturing semantic relationships and contextual nuances. Their impact on various NLP tasks includes:\n",
        "\n",
        "**1. Enhanced Semantic Understanding**: Word embeddings map words with similar meanings to proximate vectors in a continuous vector space, enabling models to grasp semantic relationships and perform tasks like sentiment analysis and topic modeling more effectively.\n",
        "\n",
        "**2. Improved Model Performance**: Incorporating word embeddings has been shown to enhance the performance of NLP tasks such as text classification, sentiment analysis, and machine translation by providing richer representations of words.\n",
        "\n",
        "**3. Dimensionality Reduction**: By representing words in a lower-dimensional space, word embeddings reduce the computational complexity of NLP models, leading to faster training times and more efficient processing.\n",
        "\n",
        "**4. Transfer Learning**: Pre-trained word embeddings can be fine-tuned for specific tasks, allowing models to leverage existing knowledge and adapt to new domains with limited labeled data.\n",
        "\n",
        "**5. Contextual Awareness**: Advanced embeddings capture context, enabling models to distinguish between different meanings of a word based on its usage, which is crucial for tasks like named entity recognition and question answering.\n",
        "\n",
        "# 33) What is the purpose of using lemmatization in text preprocessing?\n",
        "**Ans:**\n",
        "\n",
        "**Purpose of Lemmatization in Text Preprocessing:**\n",
        "\n",
        "1. **Standardization of Words**: Lemmatization reduces inflected or derived words to a common base form. For example, \"running\" and \"ran\" are both lemmatized to \"run.\" This standardization ensures that different forms of a word are treated uniformly during analysis.\n",
        "\n",
        "2. **Reduction of Vocabulary Size**: By consolidating various forms of a word into a single lemma, lemmatization decreases the number of unique tokens in the text. This reduction simplifies the complexity of the data and enhances the efficiency of subsequent NLP tasks.\n",
        "\n",
        "3. **Improved Accuracy in NLP Tasks**: Lemmatization enhances the performance of various NLP applications, including information retrieval, text summarization, and machine translation, by ensuring that words are analyzed in their base forms, leading to more accurate interpretations.\n",
        "\n",
        "4. **Enhanced Semantic Analysis**: By considering the context and part of speech, lemmatization helps in accurately capturing the meaning of words, which is essential for tasks like sentiment analysis and topic modeling.\n"
      ],
      "metadata": {
        "id": "OM8LAJr-TJXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "R6CoQr_h1dqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) How can you perform word tokenization using NLTK?"
      ],
      "metadata": {
        "id": "UlXjJ3b3uems"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "rDtpEvrpujlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # Punkt is a pre-trained tokenizer model\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"This is an example sentence.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "m3X9ogpYuw8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) How can you perform sentence tokenization using NLTK?"
      ],
      "metadata": {
        "id": "n3OhzIWNu5RD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # Punkt is a pre-trained tokenizer model\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"Hello there! How are you doing today? I hope you're having a great day.\"\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "HqPtz3vLu-Bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) How can you remove stopwords from a sentence?"
      ],
      "metadata": {
        "id": "xIeivxHZvLDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "buymaFrtvN5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample sentence\n",
        "sentence = \"This is an example showing how to remove stopwords from a sentence.\"\n",
        "\n",
        "# Tokenize the sentence into words\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "# Get the list of English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Filter out the stopwords\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Join the filtered words back into a sentence\n",
        "filtered_sentence = ' '.join(filtered_words)\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "dxXJ27rsvVID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) How can you perform stemming on a word?"
      ],
      "metadata": {
        "id": "cypgMHYNvXlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "wZZvzdKBvdUp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"running\", \"runner\", \"ran\", \"runs\"]\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "print(stemmed_words)"
      ],
      "metadata": {
        "id": "KwqDOQ13vqfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) How can you perform lemmatization on a word?"
      ],
      "metadata": {
        "id": "L7ia8GZtvwUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "id": "vz_o9lBzvy0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "tgAxYT9fwW4e"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word = \"running\"\n",
        "lemma = lemmatizer.lemmatize(word, pos='v')  # 'v' indicates verb\n",
        "print(lemma)"
      ],
      "metadata": {
        "id": "oZF1vVWSwac4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) How can you normalize a text by converting it to lowercase and removing punctuation?"
      ],
      "metadata": {
        "id": "A6FMcrdawmuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "-BAQAh5Rwu5j"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text\n",
        "\n",
        "sample_text = \"Hello, World! This is an example: text normalization.\"\n",
        "normalized_text = normalize_text(sample_text)\n",
        "print(normalized_text)\n"
      ],
      "metadata": {
        "id": "b0m7WgQpxa1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) How can you create a co-occurrence matrix for words in a corpus?"
      ],
      "metadata": {
        "id": "_O5GdYBXxfHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "corpus = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Never jump over the lazy dog quickly.\",\n",
        "    \"The fox is quick and the dog is lazy.\"\n",
        "]\n",
        "\n",
        "# Fit and transform the corpus\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute the co-occurrence matrix\n",
        "cooccurrence_matrix = (X.T * X).toarray()\n",
        "\n",
        "# Set diagonal to zero to ignore self-co-occurrences\n",
        "np.fill_diagonal(cooccurrence_matrix, 0)\n",
        "\n",
        "# Get the vocabulary\n",
        "vocabulary = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "import pandas as pd\n",
        "cooccurrence_df = pd.DataFrame(cooccurrence_matrix, index=vocabulary, columns=vocabulary)\n",
        "print(cooccurrence_df)\n"
      ],
      "metadata": {
        "id": "Ra15qzDXxnVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8) How can you apply a regular expression to extract all email addresses from a text?"
      ],
      "metadata": {
        "id": "v7OLow__yeam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"\n",
        "Please contact us at support@example.com for assistance.\n",
        "You can also reach out to john.doe123@subdomain.example.co.uk for more information.\n",
        "\"\"\"\n",
        "\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "email_addresses = re.findall(email_pattern, text)\n",
        "\n",
        "print(email_addresses)"
      ],
      "metadata": {
        "id": "FAIt72WUyu0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) How can you perform word embedding using Word2Vec?"
      ],
      "metadata": {
        "id": "qnWNdp98zGWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "7QkYdRbAzNYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "sentences = [\n",
        "    ['this', 'is', 'the', 'first', 'sentence'],\n",
        "    ['and', 'this', 'is', 'the', 'second', 'sentence'],\n",
        "]\n",
        "\n",
        "# Initialize the Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)  # sg=0 for CBOW, sg=1 for Skip-gram\n",
        "\n",
        "# Train the model\n",
        "model.save(\"word2vec.model\")\n"
      ],
      "metadata": {
        "id": "2fP5Y4WZzauY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = Word2Vec.load(\"word2vec.model\")\n",
        "\n",
        "# Access the vector for a specific word\n",
        "word_vector = model.wv['sentence']\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "id": "czGSZky6zfdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find words similar to 'sentence'\n",
        "similar_words = model.wv.most_similar('sentence', topn=5)\n",
        "print(similar_words)\n"
      ],
      "metadata": {
        "id": "3oZRyYsj0BEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) How can you use Doc2Vec to embed documents?"
      ],
      "metadata": {
        "id": "-284EyP60eOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n"
      ],
      "metadata": {
        "id": "d9Atq7ya0OQC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "documents = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "tagged_documents = [TaggedDocument(words=doc.split(), tags=[str(i)]) for i, doc in enumerate(documents)]\n",
        "\n",
        "\n",
        "model = Doc2Vec(vector_size=100, window=2, min_count=1, workers=4, epochs=100)\n",
        "model.build_vocab(tagged_documents)\n",
        "model.train(tagged_documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "\n",
        "# Save the model\n",
        "model.save(\"doc2vec.model\")\n",
        "\n",
        "# Load the model\n",
        "model = Doc2Vec.load(\"doc2vec.model\")\n",
        "\n",
        "new_document = \"This is a new document.\"\n",
        "new_vector = model.infer_vector(new_document.split())\n",
        "print(new_vector)\n"
      ],
      "metadata": {
        "id": "9faheuF20l9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11) How can you perform part-of-speech tagging?"
      ],
      "metadata": {
        "id": "wVbvjkIA0x9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "text = \"Natural language processing is fascinating.\"\n",
        "words = word_tokenize(text)\n",
        "\n",
        "pos_tags = pos_tag(words)\n",
        "print(pos_tags)\n"
      ],
      "metadata": {
        "id": "7-eDxrgx2SJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12) How can you find the similarity between two sentences using cosine similarity?"
      ],
      "metadata": {
        "id": "WwrrPM_G2zr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Sample sentences\n",
        "sentence1 = \"Natural language processing is fascinating.\"\n",
        "sentence2 = \"I find natural language processing quite interesting.\"\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Combine the sentences into a list\n",
        "sentences = [sentence1, sentence2]\n",
        "\n",
        "# Fit and transform the sentences into TF-IDF vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Compute the cosine similarity between the two sentences\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "\n",
        "print(f\"Cosine Similarity: {cosine_sim[0][0]}\")\n"
      ],
      "metadata": {
        "id": "Ev1RQ4Ve3ERV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13) How can you extract named entities from a sentence?"
      ],
      "metadata": {
        "id": "ZgODE4Jh3OUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('maxent_ne_chunker_tab')"
      ],
      "metadata": {
        "id": "VdXTNdhd3Zqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Apple Inc. is planning to open a new store in New York City on January 1, 2025.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "tagged_words = pos_tag(words)\n",
        "\n",
        "named_entities = ne_chunk(tagged_words)\n",
        "\n",
        "named_entities_list = []\n",
        "for subtree in named_entities:\n",
        "    if isinstance(subtree, nltk.Tree):\n",
        "        entity = \" \".join([word for word, tag in subtree])\n",
        "        entity_type = subtree.label()\n",
        "        named_entities_list.append((entity, entity_type))\n",
        "print(named_entities_list)\n"
      ],
      "metadata": {
        "id": "BI1XCd-H3dyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14) How can you split a large document into smaller chunks of text?"
      ],
      "metadata": {
        "id": "-8l8dXWo3mZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!split -l 1000 largefile.txt smallfile_\n",
        "# This command splits largefile.txt into multiple files, each containing 1,000 lines, with filenames starting with smallfile_."
      ],
      "metadata": {
        "id": "KminkCyR4BwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!split -b 10m largefile.txt smallfile_\n",
        "# This splits the file into chunks of 10 megabytes each."
      ],
      "metadata": {
        "id": "22oOzFUm4f7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15) How can you calculate the TF-IDF (Term Frequency - Inverse Document Frequency) for a set of documents?"
      ],
      "metadata": {
        "id": "KExzmWSM4n85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"The cat sat on the mat.\",\n",
        "    \"The dog sat on the log.\",\n",
        "    \"Cats and dogs are pets.\"\n",
        "]\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the documents\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense format and display\n",
        "tfidf_dense = tfidf_matrix.todense()\n",
        "print(tfidf_dense)\n"
      ],
      "metadata": {
        "id": "wqPd_wdh435u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 16) How can you apply tokenization, stopword removal, and stemming in one go?"
      ],
      "metadata": {
        "id": "UqPEJV1b5ADs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "9qaU7qvx5Jyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove punctuation\n",
        "    words = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    # Stopword removal\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_words = [stemmer.stem(word) for word in filtered_words]\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog!\"\n",
        "processed_text = preprocess_text(sample_text)\n",
        "print(processed_text)\n"
      ],
      "metadata": {
        "id": "Xfquy2c-5Mdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 17) How can you visualize the frequency distribution of words in a sentence?"
      ],
      "metadata": {
        "id": "hhU2pHdS5REP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download necessary NLTK data files\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ojWiEqG5Zuq",
        "outputId": "95b0ed59-b4ff-4781-93d3-284488dc15f6"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog. The dog was not amused.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Plot the frequency distribution for the 10 most common words\n",
        "freq_dist.plot(10, title='Word Frequency Distribution')\n"
      ],
      "metadata": {
        "id": "6LqpyIKr5ewA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I1RW5Vo75pnS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}