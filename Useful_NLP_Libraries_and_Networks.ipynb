{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) What is NLTK?\n",
        "**Ans:** NLTK, or Natural Language Toolkit, is a comprehensive Python library designed for natural language processing (NLP) tasks. It offers a suite of text processing libraries for activities such as classification, tokenization, stemming, tagging, parsing, and semantic reasoning. Additionally, NLTK provides easy access to over 50 corpora and lexical resources, including WordNet, making it a valuable tool for both educational and research purposes in computational linguistics and NLP.\n",
        "# 2) What is SpaCy and how does it differ from NLTK?\n",
        "**Ans:** spaCy is an open-source Python library designed for advanced natural language processing (NLP) tasks. Developed by Explosion AI, it emphasizes efficiency and ease of use, making it particularly suitable for production environments. spaCy provides pre-trained models for various languages and supports tasks such as tokenization, part-of-speech tagging, named entity recognition, and dependency parsing.\n",
        "\n",
        "**Key Differences Between NLTK and spaCy:**\n",
        "\n",
        "- **Purpose and Design:**\n",
        "  - **NLTK** is a comprehensive toolkit aimed at education and research, offering a wide range of algorithms and linguistic data.\n",
        "  - **spaCy** is engineered for industrial use, focusing on performance and providing streamlined solutions for common NLP tasks.\n",
        "\n",
        "- **Performance and Speed:**\n",
        "  - **spaCy** is optimized for speed, efficiently handling large volumes of text, which is advantageous in real-time applications.\n",
        "  - **NLTK** may be slower when processing substantial datasets due to its extensive range of functionalities.\n",
        "\n",
        "- **Flexibility and Customization:**\n",
        "  - **NLTK** offers a diverse set of tools and algorithms, allowing for detailed customization and experimentation, beneficial for research purposes.\n",
        "  - **spaCy** provides a more straightforward API with pre-trained models, facilitating quick implementation but offering less flexibility for customization.\n",
        "\n",
        "- **Language Support:**\n",
        "  - **NLTK** supports a broad spectrum of languages, making it suitable for multilingual projects.\n",
        "  - **spaCy** offers support for multiple languages, though its range is narrower compared to NLTK.\n",
        "\n",
        "- **Ease of Use:**\n",
        "  - **spaCy** is praised for its user-friendly interface and well-documented functions, enabling developers to implement NLP features efficiently.\n",
        "  - **NLTK**'s extensive features can present a steeper learning curve, potentially requiring more effort to achieve similar tasks.\n",
        "\n",
        "# 3) What is the purpose of TextBlob in NLP?\n",
        "**Ans:** TextBlob is a Python library used in Natural Language Processing (NLP) to simplify common text-processing tasks. It is built on top of libraries like NLTK and provides an easy-to-use API for beginners and developers who need quick and efficient text analysis.\n",
        "\n",
        "### Key Purposes of TextBlob:\n",
        "1. **Sentiment Analysis**:\n",
        "   - Analyzes the polarity (positive/negative) and subjectivity of text.\n",
        "   - Example: `\"I love Python!\"` â†’ Polarity: 0.8 (positive), Subjectivity: 0.9.\n",
        "\n",
        "2. **Text Classification**:\n",
        "   - Categorizes text into predefined labels (e.g., spam or non-spam).\n",
        "\n",
        "3. **Part-of-Speech (POS) Tagging**:\n",
        "   - Identifies grammatical roles of words (noun, verb, adjective, etc.).\n",
        "\n",
        "4. **Tokenization**:\n",
        "   - Splits text into words or sentences.\n",
        "\n",
        "5. **Named Entity Recognition (NER)**:\n",
        "   - Identifies entities like names, dates, and locations.\n",
        "\n",
        "6. **Spelling Correction**:\n",
        "   - Automatically detects and fixes spelling errors.\n",
        "\n",
        "7. **Language Translation**:\n",
        "   - Translates text between supported languages.\n",
        "\n",
        "8. **Text Preprocessing**:\n",
        "   - Performs lemmatization (converting words to their base form) and noun phrase extraction.\n",
        "\n",
        "# 4) What is Stanford NLP?\n",
        "**Ans:** Stanford NLP refers to a suite of Natural Language Processing tools and resources developed by the Stanford Natural Language Processing Group. These tools are designed to help computers understand, interpret, and generate human language. They are widely used in both academic research and industry applications.\n",
        "\n",
        "# 5) Explain what Recurrent Neural Networks (RNN) are?\n",
        "**Ans:** A Recurrent Neural Network (RNN) is a class of artificial neural networks designed to process sequential data by utilizing internal memory to capture information about previous inputs. This architecture enables RNNs to model temporal dynamics and dependencies within sequences, making them particularly effective for tasks where context and order are crucial.\n",
        "\n",
        "# 6) What is the main advantage of using LSTM over RNN?\n",
        "**Ans:** The main advantage of **Long Short-Term Memory (LSTM)** networks over traditional **Recurrent Neural Networks (RNNs)** is their ability to effectively capture and maintain long-term dependencies within sequential data. This capability addresses the **vanishing gradient problem** commonly encountered in standard RNNs, where the influence of earlier inputs diminishes exponentially as the sequence length increases, hindering the learning of long-range patterns.\n",
        "\n",
        "**Key Advantages of LSTMs:**\n",
        "\n",
        "- **Mitigation of Vanishing Gradient Problem:** LSTMs incorporate specialized structures known as gates (input, output, and forget gates) that regulate the flow of information, allowing them to preserve relevant information over extended periods. This design enables LSTMs to maintain gradients more effectively during backpropagation, facilitating the learning of long-term dependencies.\n",
        "\n",
        "- **Enhanced Memory Capabilities:** The internal memory cell in LSTMs allows them to store and retrieve information over long sequences, making them particularly suited for tasks that require understanding context over extended durations, such as language modeling and time-series prediction.\n",
        "\n",
        "- **Improved Learning Efficiency:** By effectively managing long-term dependencies, LSTMs can converge more quickly during training compared to traditional RNNs, especially in complex sequence modeling tasks.\n",
        "\n",
        "# 7) What are Bi-directional LSTMs, and how do they differ from standard LSTMs?\n",
        "**Ans:** **Bidirectional Long Short-Term Memory (BiLSTM)** networks are an extension of standard LSTM architectures designed to capture dependencies in sequential data from both past (previous time steps) and future (subsequent time steps) contexts. This bidirectional processing enables BiLSTMs to gain a more comprehensive understanding of the sequence, which is particularly beneficial in tasks where context from both directions is essential.\n",
        "\n",
        "**Key Differences Between Standard LSTMs and BiLSTMs:**\n",
        "\n",
        "1. **Directional Processing:**\n",
        "   - *Standard LSTM:* Processes data in a single direction, typically from the beginning to the end of the sequence, capturing information solely from past to future.\n",
        "   - *BiLSTM:* Processes data in both forward and backward directions. It consists of two LSTM layers: one processes the sequence from start to end (forward), and the other processes it from end to start (backward). The outputs from both layers are then combined, allowing the model to consider context from both past and future.\n",
        "\n",
        "2. **Contextual Understanding:**\n",
        "   - *Standard LSTM:* Utilizes information up to the current time step, which may limit understanding in cases where future context is relevant.\n",
        "   - *BiLSTM:* Incorporates information from both preceding and succeeding time steps, providing a more holistic understanding of the sequence. This is particularly advantageous in natural language processing tasks, where the meaning of a word can depend on both its preceding and following words.\n",
        "\n",
        "3. **Performance in Sequential Tasks:**\n",
        "   - *Standard LSTM:* Effective in modeling sequences with dependencies primarily in one direction.\n",
        "   - *BiLSTM:* Often outperforms standard LSTMs in tasks requiring understanding of context from both directions, such as named entity recognition, machine translation, and speech recognition, due to its ability to capture bidirectional dependencies.\n",
        "\n",
        "# 8) What is the purpose of a Stacked LSTM?\n",
        "**Ans:** A **Stacked Long Short-Term Memory (LSTM)** network is an architecture where multiple LSTM layers are layered on top of each other, with each layer's output serving as the input to the subsequent layer. This design enables the model to capture complex patterns and hierarchical representations within sequential data, enhancing its ability to understand intricate temporal dynamics.\n",
        "\n",
        "**Purpose of Stacked LSTMs:**\n",
        "\n",
        "- **Hierarchical Feature Extraction:** Each LSTM layer in the stack can learn representations at varying levels of abstraction. Lower layers might capture simple patterns, while higher layers can detect more complex structures by combining features from preceding layers.\n",
        "\n",
        "- **Modeling Complex Temporal Dependencies:** Stacking LSTM layers allows the network to capture intricate temporal relationships in data, which is particularly beneficial for tasks like language modeling, speech recognition, and time-series forecasting.\n",
        "\n",
        "- **Enhanced Learning Capacity:** By increasing the depth of the network through stacking, the model's capacity to learn and represent complex functions improves, potentially leading to better performance on challenging tasks.\n",
        "\n",
        "# 9) How does a GRU (Gated Recurrent Unit) differ from an LSTM?\n",
        "**Ans:** **Gated Recurrent Units (GRUs)** and **Long Short-Term Memory (LSTM)** networks are both advanced types of recurrent neural networks (RNNs) designed to capture long-term dependencies in sequential data. While they share the common goal of mitigating issues like the vanishing gradient problem inherent in traditional RNNs, they differ in architecture and complexity.\n",
        "\n",
        "**Key Differences Between GRUs and LSTMs:**\n",
        "\n",
        "1. **Gate Mechanisms:**\n",
        "   - *LSTM:* Employs three gatesâ€”**input**, **forget**, and **output**â€”to regulate the flow of information into, within, and out of the cell. This intricate gating system allows LSTMs to control memory content meticulously.\n",
        "   - *GRU:* Utilizes two gatesâ€”**reset** and **update**â€”which combine the functionalities of LSTM's gates into a more streamlined architecture. The update gate in GRUs serves a combined role similar to the input and forget gates in LSTMs, simplifying the model's structure.\n",
        "\n",
        "2. **Memory Cell:**\n",
        "   - *LSTM:* Contains a distinct memory cell that maintains information over time, separate from the hidden state.\n",
        "   - *GRU:* Merges the memory cell and hidden state into a single entity, leading to a more compact model with fewer parameters.\n",
        "\n",
        "3. **Parameter Complexity:**\n",
        "   - *LSTM:* Generally has a higher number of parameters due to its three-gate structure and separate memory cell, which can result in increased computational requirements.\n",
        "   - *GRU:* With fewer gates and a unified state, GRUs have fewer parameters, potentially leading to faster training times and reduced computational load.\n",
        "\n",
        "4. **Performance:**\n",
        "   - *LSTM:* Tends to perform better on complex tasks requiring learning of intricate temporal dynamics, owing to its elaborate gating mechanisms.\n",
        "   - *GRU:* Often achieves comparable performance to LSTMs on various tasks, sometimes outperforming LSTMs on less complex datasets due to its simpler architecture.\n",
        "\n",
        "# 10) What are the key features of NLTK's tokenization process?\n",
        "**Ans:** The **Natural Language Toolkit (NLTK)** offers a comprehensive suite of tokenization tools designed to segment text into smaller units, such as words or sentences, facilitating various natural language processing (NLP) tasks. Key features of NLTK's tokenization process include:\n",
        "\n",
        "- **Sentence Tokenization:** NLTK provides the `sent_tokenize` function, which divides text into individual sentences. This is particularly useful for tasks that require sentence-level analysis.\n",
        "\n",
        "- **Word Tokenization:** The `word_tokenize` function splits sentences into words and punctuation, enabling word-level analysis. This function handles punctuation and contractions effectively, ensuring accurate tokenization.\n",
        "\n",
        "- **Punkt Tokenizer:** NLTK includes the Punkt tokenizer, an unsupervised algorithm that segments text into a list of sentences by building a model for abbreviation words, collocations, and words that start sentences. This is particularly useful for languages with complex sentence structures.\n",
        "\n",
        "- **Whitespace Tokenizer:** For simpler tokenization needs, NLTK offers the `WhitespaceTokenizer`, which divides text based on whitespace. This is useful when working with well-formatted text where tokens are separated by spaces.\n",
        "\n",
        "- **Token Span Identification:** NLTK tokenizers can produce token spans, represented as tuples of integers that indicate the start and end positions of tokens within the original text. This feature supports efficient comparison of tokenizers and is useful for tasks that require alignment between tokenized and original text.\n",
        "\n",
        "- **Customization and Extensibility:** NLTK's tokenization module is highly customizable, allowing users to define their own tokenization rules or modify existing ones to suit specific requirements. This flexibility is beneficial for processing domain-specific texts or languages with unique tokenization needs.\n",
        "\n",
        "# 11) How do you perform named entity recognition (NER) using SpaCy?\n",
        "**Ans:** Named Entity Recognition (NER) is a crucial task in Natural Language Processing (NLP) that involves identifying and classifying entitiesâ€”such as persons, organizations, locations, dates, and moreâ€”within a text. **spaCy** is a robust open-source NLP library in Python that offers efficient and straightforward methods for performing NER.\n",
        "\n",
        "**Steps to Perform NER Using spaCy:**\n",
        "\n",
        "1. **Install spaCy and Download the Language Model:**\n",
        "   Ensure that spaCy is installed in your environment. Additionally, download the appropriate language model, such as `en_core_web_sm` for English.\n",
        "\n",
        "   ```bash\n",
        "   pip install spacy\n",
        "   python -m spacy download en_core_web_sm\n",
        "   ```\n",
        "\n",
        "2. **Load the Language Model:**\n",
        "   Begin by importing spaCy and loading the pre-trained language model.\n",
        "\n",
        "   ```python\n",
        "   import spacy\n",
        "   nlp = spacy.load(\"en_core_web_sm\")\n",
        "   ```\n",
        "\n",
        "3. **Process the Text:**\n",
        "   Pass the text through the `nlp` pipeline to create a `Doc` object, which includes linguistic annotations.\n",
        "\n",
        "   ```python\n",
        "   text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "   doc = nlp(text)\n",
        "   ```\n",
        "\n",
        "4. **Extract Named Entities:**\n",
        "   Iterate over the `ents` property of the `Doc` object to access the identified entities, along with their labels.\n",
        "\n",
        "   ```python\n",
        "   for ent in doc.ents:\n",
        "       print(ent.text, ent.label_)\n",
        "   ```\n",
        "\n",
        "   This will output:\n",
        "   ```\n",
        "   Apple ORG\n",
        "   U.K. GPE\n",
        "   $1 billion MONEY\n",
        "   ```\n",
        "\n",
        "# 12) What is Word2Vec and how does it represent words?\n",
        "**Ans:** **Word2Vec** is a technique in Natural Language Processing (NLP) that transforms words into continuous vector representations, known as word embeddings. These embeddings capture the semantic and syntactic relationships between words, enabling machines to understand and process human language more effectively.\n",
        "\n",
        "**How Word2Vec Represents Words:**\n",
        "\n",
        "- **Vector Space Representation:** Word2Vec maps each word in a corpus to a high-dimensional vector. Words with similar meanings or contexts are positioned close to each other in this vector space, facilitating the capture of semantic relationships.\n",
        "\n",
        "- **Contextual Learning:** The model learns word representations by analyzing the context in which words appear. It assumes that words used in similar contexts have similar meanings, a principle known as the distributional hypothesis.\n",
        "\n",
        "- **Training Architectures:** Word2Vec employs two primary architectures to learn word embeddings:\n",
        "\n",
        "  - **Continuous Bag of Words (CBOW):** Predicts a target word based on its surrounding context words. This approach is faster and more efficient for frequent words.\n",
        "\n",
        "  - **Skip-gram:** Uses a target word to predict its surrounding context words. This method is particularly effective for capturing relationships involving rare words.\n",
        "\n",
        "# 13) Explain the difference between Bag of Words (BoW) and Word2Vec?\n",
        "**Ans:** **Bag of Words (BoW)** and **Word2Vec** are two fundamental techniques in Natural Language Processing (NLP) for representing text data, each with distinct methodologies and applications.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "- **Contextual Awareness:** BoW treats words as independent entities, while Word2Vec considers the context in which words appear, capturing semantic relationships.\n",
        "\n",
        "- **Dimensionality:** BoW often results in high-dimensional, sparse vectors, whereas Word2Vec produces low-dimensional, dense vectors.\n",
        "\n",
        "- **Application Suitability:** BoW is suitable for tasks where word frequency is crucial, such as text classification. In contrast, Word2Vec excels in tasks requiring semantic understanding, like sentiment analysis and machine translation.\n",
        "\n",
        "\n",
        "# 14) How does TextBlob handle sentiment analysis?\n",
        "**Ans:** **TextBlob** is a Python library that simplifies text processing tasks, including **sentiment analysis**. It evaluates the sentiment of a given text by analyzing the polarity and subjectivity of the words and sentences within it.\n",
        "\n",
        "**How TextBlob Handles Sentiment Analysis:**\n",
        "\n",
        "1. **Polarity:** TextBlob assigns a polarity score to the text, ranging from -1 to 1. A score closer to -1 indicates a negative sentiment, while a score closer to 1 indicates a positive sentiment. A score around 0 suggests a neutral sentiment.\n",
        "\n",
        "2. **Subjectivity:** It also calculates a subjectivity score between 0 and 1. Scores closer to 0 imply that the text is more factual, whereas scores closer to 1 indicate that the text is more opinion-based.\n",
        "\n",
        "# 15) How would you implement text preprocessing using NLTK?\n",
        "**Ans:**\n",
        "\n",
        "**Implementing Text Preprocessing with NLTK:**\n",
        "\n",
        "1. **Installation and Setup:**\n",
        "   First, install NLTK and download the necessary datasets:\n",
        "\n",
        "   ```bash\n",
        "   pip install nltk\n",
        "   ```\n",
        "\n",
        "   ```python\n",
        "   import nltk\n",
        "   nltk.download('punkt')\n",
        "   nltk.download('stopwords')\n",
        "   nltk.download('wordnet')\n",
        "   ```\n",
        "\n",
        "2. **Tokenization:**\n",
        "   Tokenization involves splitting text into smaller units, such as words or sentences.\n",
        "\n",
        "   - **Word Tokenization:**\n",
        "\n",
        "     ```python\n",
        "     from nltk.tokenize import word_tokenize\n",
        "\n",
        "     text = \"Hello, world! Welcome to NLP with NLTK.\"\n",
        "     words = word_tokenize(text)\n",
        "     print(words)\n",
        "     ```\n",
        "\n",
        "     Output:\n",
        "     ```\n",
        "     ['Hello', ',', 'world', '!', 'Welcome', 'to', 'NLP', 'with', 'NLTK', '.']\n",
        "     ```\n",
        "\n",
        "   - **Sentence Tokenization:**\n",
        "\n",
        "     ```python\n",
        "     from nltk.tokenize import sent_tokenize\n",
        "\n",
        "     text = \"Hello, world! Welcome to NLP with NLTK. Let's explore text preprocessing.\"\n",
        "     sentences = sent_tokenize(text)\n",
        "     print(sentences)\n",
        "     ```\n",
        "\n",
        "     Output:\n",
        "     ```\n",
        "     ['Hello, world!', 'Welcome to NLP with NLTK.', 'Let's explore text preprocessing.']\n",
        "     ```\n",
        "\n",
        "3. **Lowercasing:**\n",
        "   Converting all text to lowercase ensures uniformity.\n",
        "\n",
        "   ```python\n",
        "   text = text.lower()\n",
        "   ```\n",
        "\n",
        "4. **Removing Punctuation and Numbers:**\n",
        "   Eliminating punctuation and numbers can be achieved using regular expressions.\n",
        "\n",
        "   ```python\n",
        "   import re\n",
        "\n",
        "   text = re.sub(r'[^a-z\\s]', '', text)\n",
        "   ```\n",
        "\n",
        "5. **Removing Stop Words:**\n",
        "   Stop words are common words that may not contribute significant meaning.\n",
        "\n",
        "   ```python\n",
        "   from nltk.corpus import stopwords\n",
        "\n",
        "   stop_words = set(stopwords.words('english'))\n",
        "   words = [word for word in words if word not in stop_words]\n",
        "   print(words)\n",
        "   ```\n",
        "\n",
        "   Output:\n",
        "   ```\n",
        "   ['hello', 'world', 'welcome', 'nlp', 'nltk', 'lets', 'explore', 'text', 'preprocessing']\n",
        "   ```\n",
        "\n",
        "6. **Stemming:**\n",
        "   Stemming reduces words to their root form.\n",
        "\n",
        "   ```python\n",
        "   from nltk.stem import PorterStemmer\n",
        "\n",
        "   stemmer = PorterStemmer()\n",
        "   stemmed_words = [stemmer.stem(word) for word in words]\n",
        "   print(stemmed_words)\n",
        "   ```\n",
        "\n",
        "   Output:\n",
        "   ```\n",
        "   ['hello', 'world', 'welcom', 'nlp', 'nltk', 'let', 'explor', 'text', 'preprocess']\n",
        "   ```\n",
        "\n",
        "7. **Lemmatization:**\n",
        "   Lemmatization converts words to their base or dictionary form.\n",
        "\n",
        "   ```python\n",
        "   from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "   lemmatizer = WordNetLemmatizer()\n",
        "   lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "   print(lemmatized_words)\n",
        "   ```\n",
        "\n",
        "   Output:\n",
        "   ```\n",
        "   ['hello', 'world', 'welcome', 'nlp', 'nltk', 'let', 'explore', 'text', 'preprocessing']\n",
        "   ```\n",
        "\n",
        "# 16) How do you train a custom NER model using SpaCy?\n",
        "**Ans:** Training a custom Named Entity Recognition (NER) model using **spaCy** involves several key steps: preparing annotated training data, setting up the training pipeline, and fine-tuning the model. Here's a structured approach to guide you through the process:\n",
        "\n",
        "**1. Install spaCy and Download a Pre-trained Model:**\n",
        "\n",
        "Begin by installing spaCy and downloading a pre-trained model to serve as the base for your custom NER model:\n",
        "\n",
        "```bash\n",
        "pip install spacy\n",
        "python -m spacy download en_core_web_sm\n",
        "```\n",
        "\n",
        "**2. Prepare Annotated Training Data:**\n",
        "\n",
        "Your training data should consist of texts annotated with the entities you wish to recognize. Each text should be paired with a dictionary containing the text and its corresponding annotations. For example:\n",
        "\n",
        "```python\n",
        "TRAINING_DATA = [\n",
        "    (\"Apple is looking at buying U.K. startup for $1 billion\", {\"entities\": [(0, 5, \"ORG\"), (27, 30, \"GPE\"), (44, 45, \"MONEY\")]}),\n",
        "    (\"Autonomous cars shift insurance liability toward manufacturers\", {\"entities\": [(0, 10, \"PRODUCT\"), (41, 55, \"ORG\")]}),\n",
        "]\n",
        "```\n",
        "\n",
        "In this example, \"Apple\" is labeled as an organization (`ORG`), \"U.K.\" as a geopolitical entity (`GPE`), and \"$1 billion\" as money (`MONEY`).\n",
        "\n",
        "**3. Load the Pre-trained Model and Create a New NER Component:**\n",
        "\n",
        "Load the pre-trained model and add a new NER component to its pipeline:\n",
        "\n",
        "```python\n",
        "import spacy\n",
        "from spacy.training.example import Example\n",
        "\n",
        "# Load the pre-trained model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create a new NER component\n",
        "ner = nlp.create_pipe(\"ner\")\n",
        "nlp.add_pipe(ner, last=True)\n",
        "```\n",
        "\n",
        "**4. Add New Entity Labels:**\n",
        "\n",
        "Add the new entity labels to the NER component:\n",
        "\n",
        "```python\n",
        "for _, annotations in TRAINING_DATA:\n",
        "    for ent in annotations.get(\"entities\"):\n",
        "        ner.add_label(ent[2])\n",
        "```\n",
        "\n",
        "**5. Prepare the Training Data:**\n",
        "\n",
        "Convert your training data into spaCy's `Example` format:\n",
        "\n",
        "```python\n",
        "# Convert training data to spaCy's Example format\n",
        "train_examples = []\n",
        "for text, annotations in TRAINING_DATA:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    train_examples.append(example)\n",
        "```\n",
        "\n",
        "**6. Train the Model:**\n",
        "\n",
        "Train the NER component using the prepared data:\n",
        "\n",
        "```python\n",
        "# Disable other pipeline components during training to avoid interference\n",
        "pipe_exceptions = [\"ner\"]\n",
        "with nlp.disable_pipes(*pipe_exceptions):\n",
        "    optimizer = nlp.begin_training()\n",
        "    for epoch in range(30):\n",
        "        losses = {}\n",
        "        # Shuffle the training data\n",
        "        random.shuffle(train_examples)\n",
        "        # Update the model with each example\n",
        "        for example in train_examples:\n",
        "            nlp.update([example], drop=0.5, losses=losses)\n",
        "        print(f\"Epoch {epoch} - Losses: {losses}\")\n",
        "```\n",
        "\n",
        "In this training loop, the model is updated over 30 epochs, with a dropout rate of 0.5 to prevent overfitting.\n",
        "\n",
        "**7. Save the Trained Model:**\n",
        "\n",
        "After training, save the model for future use:\n",
        "\n",
        "```python\n",
        "nlp.to_disk(\"path_to_save_model\")\n",
        "```\n",
        "\n",
        "**8. Evaluate the Model:**\n",
        "\n",
        "To evaluate the model's performance, you can use a separate test dataset with known annotations. This allows you to assess the model's accuracy and make necessary adjustments.\n",
        "\n",
        "# 17) What is the role of the attention mechanism in LSTMs and GRUs?\n",
        "**Ans:** The **attention mechanism** enhances the performance of Recurrent Neural Networks (RNNs), including Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs), by enabling the model to focus on specific parts of the input sequence when making predictions. This capability is particularly beneficial for tasks involving long sequences, where traditional RNNs may struggle to capture distant dependencies due to issues like vanishing gradients.\n",
        "\n",
        "**Role of Attention in LSTMs and GRUs:**\n",
        "\n",
        "1. **Contextual Focus:** Attention allows the model to assign varying levels of importance to different parts of the input sequence. This means that when processing a particular output, the model can \"attend\" to the most relevant parts of the input, effectively capturing long-range dependencies.\n",
        "\n",
        "2. **Improved Performance:** By integrating attention mechanisms, LSTMs and GRUs can more effectively handle tasks such as machine translation, where understanding the context of the entire input sequence is crucial. This leads to more accurate and contextually appropriate outputs.\n",
        "\n",
        "3. **Dynamic Weighting:** The attention mechanism computes a set of weights that determine the significance of each input element for a given output. These weights are dynamically adjusted during training, allowing the model to learn which parts of the input are most relevant for each specific prediction.\n",
        "\n",
        "# 18) What is the difference between tokenization and lemmatization in NLP?\n",
        "**Ans:** In Natural Language Processing (NLP), **tokenization** and **lemmatization** are fundamental preprocessing steps that prepare text data for analysis.\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "- **Purpose:**\n",
        "  - *Tokenization:* Breaks text into smaller units (tokens) for analysis.\n",
        "  - *Lemmatization:* Reduces words to their base forms to standardize them.\n",
        "\n",
        "- **Process:**\n",
        "  - *Tokenization:* Splits text based on delimiters like spaces and punctuation.\n",
        "  - *Lemmatization:* Considers the word's meaning and context, often requiring part-of-speech tagging.\n",
        "\n",
        "- **Output:**\n",
        "  - *Tokenization:* Produces a list of tokens (words or sentences).\n",
        "  - *Lemmatization:* Produces the lemma of each word.\n",
        "\n",
        "# 19) How do you perform text normalization in NLP?\n",
        "**Ans:** Text normalization is a crucial preprocessing step in Natural Language Processing (NLP) that transforms text into a consistent and standardized format. This process reduces variability in the text, making it easier for models to analyze and interpret. Common techniques for text normalization include:\n",
        "\n",
        "1. **Lowercasing:** Converting all characters in the text to lowercase to ensure uniformity.\n",
        "\n",
        "2. **Removing Punctuation and Special Characters:** Eliminating non-alphanumeric characters that may not contribute to the analysis.\n",
        "\n",
        "3. **Expanding Contractions:** Replacing contractions with their full forms (e.g., \"can't\" becomes \"cannot\") to standardize expressions.\n",
        "\n",
        "4. **Removing Stop Words:** Eliminating common words (e.g., \"the,\" \"is,\" \"in\") that may not add significant meaning to the analysis.\n",
        "\n",
        "5. **Stemming:** Reducing words to their base or root form by removing prefixes and suffixes (e.g., \"running\" becomes \"run\").\n",
        "\n",
        "6. **Lemmatization:** Converting words to their base or dictionary form, considering the word's meaning and context (e.g., \"better\" becomes \"good\").\n",
        "\n",
        "# 20) What is the purpose of frequency distribution in NLP?\n",
        "**Ans:** In Natural Language Processing (NLP), a **frequency distribution** is a statistical tool used to count and analyze the frequency of itemsâ€”such as words, characters, or n-gramsâ€”within a text or a collection of texts. This analysis provides valuable insights into the structure and content of the language data.\n",
        "\n",
        "**Purpose of Frequency Distribution in NLP:**\n",
        "\n",
        "1. **Identifying Common Elements:** By examining the frequency of words or phrases, one can identify the most common elements in a text. This is particularly useful for tasks like keyword extraction, sentiment analysis, and understanding the thematic focus of a document.\n",
        "\n",
        "2. **Data Exploration:** Frequency distributions serve as a foundational step in exploratory data analysis, helping to uncover patterns, anomalies, and the overall distribution of terms within a dataset.\n",
        "\n",
        "3. **Feature Selection:** In machine learning applications, understanding the frequency of terms aids in feature selection by highlighting significant words that contribute to the predictive power of models.\n",
        "\n",
        "4. **Text Simplification:** Recognizing and removing stop wordsâ€”commonly used words that may not add significant meaningâ€”can be facilitated by analyzing their frequency, thereby streamlining text for further processing.\n",
        "\n",
        "# 21) What are co-occurrence vectors in NLP?\n",
        "**Ans:** In Natural Language Processing (NLP), co-occurrence vectors are numerical representations that capture the relationships between words based on their co-occurrence patterns within a text corpus. These vectors are constructed by analyzing the frequency with which pairs of words appear together in a specified context window.\n",
        "# 22) How is Word2Vec used to find the relationship between words?\n",
        "**Ans:** Word2Vec is a technique in Natural Language Processing (NLP) that transforms words into continuous vector representations, capturing semantic relationships between them. By analyzing large text corpora, Word2Vec learns to position words with similar meanings or contexts closer together in the vector space.\n",
        "\n",
        "**How Word2Vec Captures Word Relationships:**\n",
        "\n",
        "1. **Training Process:**\n",
        "   - **Contextual Analysis:** Word2Vec examines the context in which words appear. It considers the surrounding words (context) of a target word within a defined window size. This approach helps the model understand the semantic relationships between words based on their usage patterns.\n",
        "   - **Objective:** The model aims to predict a target word given its context (Continuous Bag of Words model) or predict the context given a target word (Skip-gram model). Through this prediction task, Word2Vec adjusts the word vectors to minimize prediction errors, effectively learning word associations.\n",
        "\n",
        "2. **Vector Representation:**\n",
        "   - **Embedding Space:** After training, each word is represented as a vector in a high-dimensional space. Words that share similar contexts are positioned closer together, reflecting their semantic similarity.\n",
        "   - **Arithmetic Operations:** Word2Vec enables arithmetic operations on word vectors that mirror linguistic relationships. For example, the vector operation \"king\" - \"man\" + \"woman\" results in a vector close to \"queen,\" illustrating the model's ability to capture analogies.\n",
        "\n",
        "# 23) How does a Bi-LSTM improve NLP tasks compared to a regular LSTM?\n",
        "**Ans:**\n",
        "\n",
        "**Key Advantages of Bi-LSTMs Over Unidirectional LSTMs:**\n",
        "\n",
        "1. **Comprehensive Context Understanding:**\n",
        "   - Unidirectional LSTMs process sequences in a single direction (typically left to right), learning from past information. In contrast, Bi-LSTMs analyze sequences in both directions, allowing them to incorporate information from both preceding and succeeding elements. This bidirectional processing is particularly beneficial for tasks where understanding the full context is crucial.\n",
        "\n",
        "2. **Improved Performance in Sequence Tagging:**\n",
        "   - Bi-LSTMs have demonstrated superior performance in sequence tagging tasks such as part-of-speech tagging, chunking, and named entity recognition. By leveraging context from both directions, Bi-LSTMs can more accurately assign labels to each element in a sequence.\n",
        "\n",
        "3. **Enhanced Sentence Modeling:**\n",
        "   - In sentence-level tasks like sentiment analysis and text classification, Bi-LSTMs capture the overall sentiment or meaning by considering the entire sentence context. This holistic understanding leads to more accurate predictions.\n",
        "\n",
        "4. **Robustness to Input Variability:**\n",
        "   - Bi-LSTMs are less sensitive to the order of input sequences, making them more robust to variations in sentence structure and word order. This flexibility is advantageous in languages with flexible word orders or in noisy text data.\n",
        "\n",
        "# 24) What is the difference between a GRU and an LSTM in terms of gate structures?\n",
        "**Ans:** In Recurrent Neural Networks (RNNs), both Long Short-Term Memory (LSTM) units and Gated Recurrent Units (GRU) utilize gating mechanisms to manage information flow and address challenges like vanishing gradients. However, they differ in the number and function of these gates.\n",
        "\n",
        "**LSTM Gate Structure:**\n",
        "\n",
        "LSTMs employ three gates:\n",
        "\n",
        "1. **Forget Gate:** Decides which information from the previous cell state should be discarded.\n",
        "2. **Input Gate:** Determines which new information is added to the cell state.\n",
        "3. **Output Gate:** Controls which part of the cell state is output to the next layer.\n",
        "\n",
        "This tri-gate system enables LSTMs to effectively manage long-term dependencies by regulating information retention and propagation.\n",
        "\n",
        "**GRU Gate Structure:**\n",
        "\n",
        "GRUs simplify this architecture by using two gates:\n",
        "\n",
        "1. **Update Gate:** Combines the functions of the input and forget gates, determining how much of the previous memory to retain and how much of the new information to incorporate.\n",
        "2. **Reset Gate:** Decides how much of the past information to forget when updating the current state.\n",
        "\n",
        "By merging the input and forget gates, GRUs reduce computational complexity while maintaining performance in capturing dependencies.\n",
        "\n",
        "# 25) How does Stanford NLPâ€™s dependency parsing work?\n",
        "**Ans:** Stanford NLP's dependency parsing analyzes the grammatical structure of sentences by identifying relationships between words, forming a tree structure where each word (except the root) depends on another. This process reveals how words are syntactically connected, aiding in understanding sentence structure and meaning.\n",
        "\n",
        "**Key Components of Stanford NLP's Dependency Parsing:**\n",
        "\n",
        "1. **Transition-Based Parsing:**\n",
        "   - Stanford's parser employs a transition-based approach, processing sentences incrementally and making decisions based on the current configuration of the stack and input buffer. This method allows for efficient parsing by reducing the complexity of considering all possible parse trees.\n",
        "\n",
        "2. **Neural Network Integration:**\n",
        "   - The parser integrates neural networks to predict parsing actions, enhancing accuracy and adaptability to various linguistic structures. This integration enables the parser to learn complex patterns in language data, improving its performance over traditional rule-based methods.\n",
        "\n",
        "3. **Universal Dependencies Framework:**\n",
        "   - Stanford NLP's dependency parser utilizes the Universal Dependencies (UD) framework, which provides a standardized set of syntactic annotations across languages. This standardization facilitates cross-linguistic parsing and analysis, making the parser versatile for multilingual applications.\n",
        "\n",
        "4. **Pipeline Architecture:**\n",
        "   - The parser operates within a pipeline that includes tokenization, part-of-speech tagging, and dependency parsing. This sequential processing ensures that each component contributes to the overall understanding of sentence structure, leading to more accurate parsing results.\n",
        "\n",
        "# 26) How does tokenization affect downstream NLP tasks?\n",
        "**Ans:** Tokenization is a fundamental step in Natural Language Processing (NLP) that involves converting raw text into a sequence of tokens, such as words or subwords. The choice of tokenization method significantly influences the performance of downstream NLP tasks, including sentiment analysis, machine translation, and named entity recognition.\n",
        "\n",
        "**Impact of Tokenization on Downstream NLP Tasks:**\n",
        "\n",
        "1. **Vocabulary Size and Coverage:**\n",
        "   - Tokenization determines the vocabulary size, which affects the model's ability to handle rare or out-of-vocabulary words. Subword tokenization methods, like Byte-Pair Encoding (BPE), can effectively manage rare words by breaking them into more frequent subword units, thereby reducing the vocabulary size and improving model efficiency.\n",
        "\n",
        "2. **Semantic Representation:**\n",
        "   - The granularity of tokenization influences how well semantic relationships are captured. For instance, breaking down named entities into individual tokens can disrupt their semantic meaning, impacting tasks like named entity recognition.\n",
        "\n",
        "3. **Task-Specific Optimization:**\n",
        "   - Tokenization can be optimized for specific downstream tasks. Joint optimization of tokenization and downstream models has been shown to improve performance by determining appropriate tokenizations tailored to the task.\n",
        "\n",
        "4. **Language-Specific Considerations:**\n",
        "   - In languages with complex morphology or scriptio continua (continuous scripts without spaces), tokenization becomes more challenging. The choice of tokenizers in such languages can significantly affect the performance of pretrained language models in downstream tasks.\n",
        "\n",
        "5. **Pre-Tokenization and Normalization:**\n",
        "   - Effective pre-tokenization and normalization processes can enhance the accuracy and efficiency of subsequent NLP tasks. Proper handling of punctuation, whitespace, and case normalization during tokenization ensures that the model receives clean and consistent input.\n",
        "\n",
        "# 27) What are some common applications of NLP?\n",
        "**Ans:** Natural Language Processing (NLP) enables machines to understand, interpret, and generate human language, leading to numerous applications across various domains. Here are some common applications of NLP:\n",
        "\n",
        "- Chatbots and Virtual Assistants\n",
        "\n",
        "- Sentiment Analysis\n",
        "\n",
        "- Machine Translation  \n",
        "\n",
        "- Speech Recognition  \n",
        "\n",
        "- Text Summarization  \n",
        "\n",
        "# 28) What are stopwords and why are they removed in NLP?\n",
        "**Ans:** In Natural Language Processing (NLP), **stopwords** are common wordsâ€”such as \"the,\" \"is,\" \"in,\" and \"and\"â€”that are often removed during text preprocessing. These words are considered to carry minimal meaningful information and are frequently used across various contexts.\n",
        "\n",
        "**Reasons for Removing Stopwords:**\n",
        "\n",
        "1. **Reducing Noise:** Eliminating stopwords helps focus on the more informative words in a text, thereby reducing noise and enhancing the quality of text analysis.\n",
        "\n",
        "2. **Improving Efficiency:** By removing these common words, the size of the text data is reduced, leading to faster processing times and more efficient analysis.\n",
        "\n",
        "3. **Enhancing Performance:** In tasks like information retrieval and text classification, removing stopwords can improve performance by allowing algorithms to concentrate on the words that carry more significant meaning.\n",
        "\n",
        "# 29) How can you implement word embeddings using Word2Vec in Python?\n",
        "**Ans:**\n",
        "\n",
        "**Implementing Word Embeddings with Word2Vec in Python:**\n",
        "\n",
        "To implement Word2Vec in Python, the `gensim` library provides an efficient and straightforward approach. Here's a step-by-step guide:\n",
        "\n",
        "1. **Install the Required Libraries:**\n",
        "\n",
        "   First, ensure that you have the necessary libraries installed:\n",
        "\n",
        "   ```bash\n",
        "   pip install gensim nltk\n",
        "   ```\n",
        "\n",
        "2. **Import the Libraries:**\n",
        "\n",
        "   ```python\n",
        "   import nltk\n",
        "   from nltk.tokenize import word_tokenize\n",
        "   from gensim.models import Word2Vec\n",
        "   ```\n",
        "\n",
        "3. **Download NLTK Resources:**\n",
        "\n",
        "   ```python\n",
        "   nltk.download('punkt')\n",
        "   ```\n",
        "\n",
        "4. **Prepare Your Text Data:**\n",
        "\n",
        "   For demonstration purposes, let's use a simple text corpus. In practice, you would use a larger and more diverse dataset.\n",
        "\n",
        "   ```python\n",
        "   text = \"Natural language processing with Word2Vec is powerful for understanding word semantics.\"\n",
        "   sentences = nltk.sent_tokenize(text)\n",
        "   tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "   ```\n",
        "\n",
        "5. **Train the Word2Vec Model:**\n",
        "\n",
        "   ```python\n",
        "   model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
        "   ```\n",
        "\n",
        "   - `vector_size`: Dimensionality of the feature vectors.\n",
        "   - `window`: Maximum distance between the current and predicted word within a sentence.\n",
        "   - `min_count`: Ignores all words with total frequency lower than this.\n",
        "   - `sg`: Skip-gram method (`sg=1`) or CBOW (`sg=0`).\n",
        "\n",
        "6. **Access Word Embeddings:**\n",
        "\n",
        "   ```python\n",
        "   word_vector = model.wv['word2vec']\n",
        "   print(word_vector)\n",
        "   ```\n",
        "\n",
        "7. **Find Similar Words:**\n",
        "\n",
        "   ```python\n",
        "   similar_words = model.wv.most_similar('word2vec', topn=5)\n",
        "   print(similar_words)\n",
        "   ```\n",
        "\n",
        "# 30) How does SpaCy handle lemmatization?\n",
        "**Ans:** SpaCy's Approach to Lemmatization:\n",
        "\n",
        "SpaCy employs a combination of rule-based methods and lookup tables to perform lemmatization:\n",
        "\n",
        "1. **Lookup Tables:** SpaCy utilizes lookup tables that map inflected forms of words to their corresponding lemmas. This method is particularly effective for handling irregular words and exceptions.\n",
        "\n",
        "\n",
        "2. **Rule-Based Methods:** For regular inflections, SpaCy applies linguistic rules to transform words into their base forms. These rules are based on part-of-speech tags and are designed to handle common morphological variations.\n",
        "\n",
        "# 31) What is the significance of RNNs in NLP tasks?\n",
        "**Ans:**\n",
        "\n",
        "**Significance of RNNs in NLP Tasks:**\n",
        "\n",
        "1. **Modeling Sequential Data:** RNNs are adept at processing sequences, such as sentences or time-series data, by maintaining hidden states that capture information about previous inputs. This capability is crucial for understanding context and relationships between words in a sentence.\n",
        "\n",
        "2. **Handling Variable-Length Inputs and Outputs:** RNNs can process inputs and produce outputs of varying lengths, making them suitable for tasks like language translation, where sentences in different languages may have different lengths.\n",
        "\n",
        "3. **Capturing Temporal Dependencies:** In tasks such as speech recognition and language modeling, RNNs can capture temporal dependencies, allowing them to understand the sequence and timing of words or sounds.\n",
        "\n",
        "4. **Bidirectional Processing:** Bidirectional RNNs (BRNNs) process data in both forward and backward directions, enabling the model to capture context from both past and future inputs. This is particularly useful in tasks like named entity recognition, where understanding the surrounding context is essential.\n",
        "\n",
        "5. **Flexibility in Architecture:** RNNs can be combined with other neural network architectures, such as Convolutional Neural Networks (CNNs) and attention mechanisms, to enhance performance in complex NLP tasks. For example, combining RNNs with attention mechanisms has led to significant improvements in machine translation and text summarization.\n",
        "\n",
        "# 32) How does word embedding improve the performance of NLP models?\n",
        "**Ans:**\n",
        "\n",
        "**How Word Embeddings Enhance NLP Model Performance:**\n",
        "\n",
        "1. **Capturing Semantic Relationships:** Word embeddings map semantically similar words to nearby points in the vector space. For instance, words like \"king\" and \"queen\" are positioned close to each other, reflecting their related meanings. This proximity allows models to recognize and leverage these relationships in tasks such as sentiment analysis and machine translation.\n",
        "\n",
        "2. **Reducing Dimensionality:** Traditional text representations, like one-hot encoding, result in sparse vectors with high dimensionality. Word embeddings condense this information into dense vectors of fixed size, reducing computational complexity and memory usage. This efficiency is particularly beneficial for large-scale NLP applications.\n",
        "\n",
        "3. **Improving Generalization:** By capturing the context and meaning of words, embeddings enable models to generalize better to unseen data. This generalization is crucial for tasks like text classification, where understanding the underlying meaning of words leads to more accurate predictions.\n",
        "\n",
        "4. **Enhancing Transfer Learning:** Pre-trained word embeddings can be fine-tuned for specific tasks, allowing models to leverage knowledge from large corpora. This transfer learning approach accelerates training and improves performance on specialized NLP tasks.\n",
        "\n",
        "5. **Facilitating Contextual Understanding:** Advanced embedding techniques, such as contextual embeddings, consider the surrounding words to determine a word's meaning in a specific context. This dynamic representation enhances the model's ability to understand polysemy and homonymy, leading to more accurate interpretations in tasks like named entity recognition.\n",
        "\n",
        "# 33) How does a Stacked LSTM differ from a single LSTM?\n",
        "**Ans:**\n",
        "\n",
        "**Key Differences Between Stacked LSTM and Single LSTM:**\n",
        "\n",
        "1. **Depth and Complexity:**\n",
        "   - *Single LSTM:* Contains a single hidden LSTM layer that processes input sequences.\n",
        "   - *Stacked LSTM:* Comprises multiple LSTM layers, allowing the model to learn hierarchical representations of data.\n",
        "\n",
        "2. **Representation Learning:**\n",
        "   - *Single LSTM:* Learns representations at a single level of abstraction.\n",
        "   - *Stacked LSTM:* Each layer can capture different levels of abstraction, enabling the model to understand more complex patterns.\n",
        "\n",
        "3. **Performance and Generalization:**\n",
        "   - *Single LSTM:* May struggle with tasks requiring the understanding of intricate patterns or long-term dependencies.\n",
        "   - *Stacked LSTM:* Better suited for complex tasks, as deeper architectures can model more intricate relationships.\n",
        "\n",
        "4. **Training Time and Computational Resources:**\n",
        "   - *Single LSTM:* Generally requires less training time and computational power.\n",
        "   - *Stacked LSTM:* Increased depth leads to longer training times and higher computational demands.\n",
        "\n",
        "# 34) What are the key differences between RNN, LSTM, and GRU?\n",
        "**Ans:** Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Gated Recurrent Units (GRUs) are foundational architectures in deep learning, particularly for processing sequential data. While they share similarities, each has distinct characteristics that influence their performance and suitability for various tasks.\n",
        "\n",
        "1. Recurrent Neural Networks (RNNs):\n",
        "\n",
        "- Structure: RNNs consist of a single layer where each neuron receives input from the previous time step, allowing information to persist across time.\n",
        "\n",
        "- Functionality: They are designed to handle sequences by maintaining a hidden state that captures information from previous inputs.\n",
        "\n",
        "- Limitations: RNNs often struggle with long-term dependencies due to issues like vanishing and exploding gradients, making it challenging to learn from distant time steps.\n",
        "\n",
        "2. Long Short-Term Memory Networks (LSTMs):\n",
        "\n",
        "- Structure: LSTMs introduce a more complex architecture with three gates: input, forget, and output gates. These gates regulate the flow of information, allowing the network to decide what to remember and what to forget.\n",
        "\n",
        "- Functionality: The gating mechanism enables LSTMs to capture long-term dependencies more effectively than standard RNNs.\n",
        "\n",
        "- Advantages: LSTMs are well-suited for tasks requiring the modeling of long-term dependencies, such as language translation and speech recognition.\n",
        "\n",
        "3. Gated Recurrent Units (GRUs):\n",
        "\n",
        "- Structure: GRUs simplify the LSTM architecture by combining the forget and input gates into a single update gate and introducing a reset gate.\n",
        "\n",
        "- Functionality: The update gate determines how much of the previous memory to retain, while the reset gate controls how much of the past information to forget.\n",
        "\n",
        "- Advantages: GRUs have fewer parameters than LSTMs, leading to faster training times and reduced computational requirements. They often perform comparably to LSTMs on various tasks.\n",
        "\n",
        "# 35) Why is the attention mechanism important in sequence-to-sequence models?\n",
        "**Ans:** In sequence-to-sequence models, the attention mechanism plays a pivotal role by enabling the model to focus on specific parts of the input sequence during the decoding process. This selective focus allows the model to dynamically weigh the importance of different input elements, leading to more accurate and contextually relevant outputs."
      ],
      "metadata": {
        "id": "Ry32LtGnKDRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice"
      ],
      "metadata": {
        "id": "lXmlimlg2Ey-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) How do you perform word tokenization using NLTK and plot a word frequency distribution?"
      ],
      "metadata": {
        "id": "xpXDyqtt2HRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk matplotlib"
      ],
      "metadata": {
        "id": "WMp5x0LZ2OEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Download the 'punkt' resource for tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "e9uVRviH-eZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "In the heart of the bustling city, there stood an ancient library forgotten by time.\n",
        "Its towering shelves, laden with dusty tomes, whispered secrets of civilizations past.\n",
        "One rainy evening, a curious child named Elara stumbled upon its grand doors, ajar and inviting.\n",
        "As she stepped inside, the scent of aged paper and ink enveloped her, and the dim glow of lanterns illuminated paths of knowledge waiting to be explored.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Compute frequency distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Plot the 30 most common words\n",
        "plt.figure(figsize=(12, 6))\n",
        "freq_dist.plot(30, cumulative=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1xhiL-CQ-jP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) How do you use SpaCy for dependency parsing of a sentence?"
      ],
      "metadata": {
        "id": "flOTa7J6BCpn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "YM7-kfkqBOBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n"
      ],
      "metadata": {
        "id": "-eFlVxVTBfSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Apple's CEO Tim Cook visited the company's headquarters in Cupertino.\"\n",
        "\n",
        "# Process the sentence\n",
        "doc = nlp(sentence)\n"
      ],
      "metadata": {
        "id": "weZSrZNnBswO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Head: {token.head.text}, Dependency: {token.dep_}\")\n"
      ],
      "metadata": {
        "id": "06G6k1H0Bvsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "\n",
        "# Render the dependency parse in a Jupyter notebook\n",
        "displacy.render(doc, style='dep', jupyter=True)\n"
      ],
      "metadata": {
        "id": "NMGEEuixBz4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) How do you use TextBlob for performing text classification based on polarity?"
      ],
      "metadata": {
        "id": "FJkefJ28B0Ti"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "\n",
        "text = \"I love sunny days, but I hate the rain.\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Get the polarity\n",
        "polarity = blob.sentiment.polarity\n",
        "print(f\"Polarity: {polarity}\")\n",
        "\n",
        "# Classify the text\n",
        "def classify_text(polarity):\n",
        "    if polarity > 0:\n",
        "        return \"Positive\"\n",
        "    elif polarity < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "sentiment = classify_text(polarity)\n",
        "print(f\"Sentiment: {sentiment}\")\n"
      ],
      "metadata": {
        "id": "q2O4J8KCtJZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) How do you extract named entities from a text using SpaCy?"
      ],
      "metadata": {
        "id": "C-sQNWRRtZQQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "\n",
        "text = \"Apple is looking at buying U.K. startup for $1 billion.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract and print named entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")\n"
      ],
      "metadata": {
        "id": "R0Moc-q-t_tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) How can you calculate TF-IDF scores for a given text using Scikit-learn?"
      ],
      "metadata": {
        "id": "TMv294hnuRam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "corpus = [\n",
        "    'The cat sat on the mat.',\n",
        "    'The dog chased the cat.',\n",
        "    'The cat climbed the tree.'\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the corpus\n",
        "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to a dense format\n",
        "dense_matrix = tfidf_matrix.todense()\n",
        "\n",
        "# Create a DataFrame with the TF-IDF scores\n",
        "df = pd.DataFrame(dense_matrix, columns=feature_names)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "CKNJLJFqubig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) How do you create a custom text classifier using NLTK's Naive Bayes classifier?"
      ],
      "metadata": {
        "id": "cZZs_I4wvZYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy as nltk_accuracy\n",
        "import random"
      ],
      "metadata": {
        "id": "R4AYGcV-veQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('movie_reviews')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "nbw9Z50qv9C-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load movie reviews from NLTK corpus\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "             for category in movie_reviews.categories()\n",
        "             for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the documents to ensure random distribution\n",
        "random.shuffle(documents)\n"
      ],
      "metadata": {
        "id": "ci7O2XBUwDq7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of all words in the movie reviews corpus\n",
        "all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "\n",
        "# Select the top 2,000 most frequent words as features\n",
        "word_features = list(all_words)[:2000]\n",
        "\n",
        "# Define a feature extractor function\n",
        "def document_features(document):\n",
        "    document_words = set(document)\n",
        "    features = {}\n",
        "    for word in word_features:\n",
        "        features[f'contains({word})'] = (word in document_words)\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "iSNTUyMlwh6r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature sets for all documents\n",
        "featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "\n",
        "# Define the training and testing split (e.g., 80% training, 20% testing)\n",
        "train_size = int(len(featuresets) * 0.8)\n",
        "train_set, test_set = featuresets[:train_size], featuresets[train_size:]\n",
        "\n",
        "# Train the Naive Bayes classifier\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Calculate and display the accuracy\n",
        "print(f'Accuracy: {nltk_accuracy(classifier, test_set) * 100:.2f}%')\n",
        "\n",
        "# Show the most informative features\n",
        "classifier.show_most_informative_features(10)\n",
        "\n",
        "\n",
        "# Function to classify new text\n",
        "def classify_review(review):\n",
        "    # Tokenize the review\n",
        "    tokens = nltk.word_tokenize(review)\n",
        "    # Extract features\n",
        "    features = document_features(tokens)\n",
        "    # Classify and return the result\n",
        "    return classifier.classify(features)\n",
        "\n",
        "# Example usage\n",
        "new_review = \"This movie was an amazing experience with stellar performances.\"\n",
        "print(f'Review: {new_review}')\n",
        "print(f'Classification: {classify_review(new_review)}')\n"
      ],
      "metadata": {
        "id": "MUUhLW5p06Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) How do you use a pre-trained model from Hugging Face for text classification?"
      ],
      "metadata": {
        "id": "naVabls81eGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "EgJTqcZd2mTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('text-classification', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "texts = [\n",
        "    \"Hugging Face's Transformers library is amazing!\",\n",
        "    \"I'm not sure how I feel about this product.\",\n",
        "    \"The movie was absolutely terrible.\"\n",
        "]\n",
        "results = classifier(texts)\n",
        "for result in results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "MuW7ka7T2qBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) How do you perform text summarization using Hugging Face transformers?"
      ],
      "metadata": {
        "id": "tstanmKg22so"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline('summarization', model='facebook/bart-large-cnn')\n",
        "texts = [\n",
        "    \"First long text document.\",\n",
        "    \"Second long text document.\",\n",
        "    \"Third long text document.\"\n",
        "]\n",
        "summaries = summarizer(texts, max_length=50, min_length=30, do_sample=False)\n",
        "for summary in summaries:\n",
        "    print(summary['summary_text'])\n"
      ],
      "metadata": {
        "id": "wk_MtLn84nzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) How can you create a simple RNN for text classification using Keras?"
      ],
      "metadata": {
        "id": "hjLKRsg45Ptw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense"
      ],
      "metadata": {
        "id": "v2ogmDHP5VSN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [\n",
        "    'I love this movie',\n",
        "    'This film was terrible',\n",
        "    'What a fantastic experience',\n",
        "    'I did not enjoy the film',\n",
        "    'Absolutely wonderful movie',\n",
        "    'The movie was okay',\n",
        "    'Not my cup of tea',\n",
        "    'An excellent film',\n",
        "    'I would not recommend this movie',\n",
        "    'Best movie ever'\n",
        "]\n",
        "\n",
        "# Corresponding labels (1 for positive, 0 for negative)\n",
        "labels = [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]\n"
      ],
      "metadata": {
        "id": "39Fwu0Sf6OdC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
        "    SimpleRNN(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7nfCOF6P6TeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10)  How do you train a Bidirectional LSTM for text classification?"
      ],
      "metadata": {
        "id": "eFVPMQXr6oJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense"
      ],
      "metadata": {
        "id": "ltukM6696ZbM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [\n",
        "    'I love this movie',\n",
        "    'This film was terrible',\n",
        "    'What a fantastic experience',\n",
        "    'I did not enjoy the film',\n",
        "    'Absolutely wonderful movie',\n",
        "    'The movie was okay',\n",
        "    'Not my cup of tea',\n",
        "    'An excellent film',\n",
        "    'I would not recommend this movie',\n",
        "    'Best movie ever'\n",
        "]\n",
        "\n",
        "# Corresponding labels (1 for positive, 0 for negative)\n",
        "labels = [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]\n"
      ],
      "metadata": {
        "id": "Z4z78AZd6m4N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
        "    Bidirectional(LSTM(32)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "Tn4v8-x-613T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11) How do you implement GRU (Gated Recurrent Unit) for text classification?"
      ],
      "metadata": {
        "id": "T0z5NeUP6_wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n"
      ],
      "metadata": {
        "id": "edrEBl2y7INz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "texts = [\n",
        "    'I love this movie',\n",
        "    'This film was terrible',\n",
        "    'What a fantastic experience',\n",
        "    'I did not enjoy the film',\n",
        "    'Absolutely wonderful movie',\n",
        "    'The movie was okay',\n",
        "    'Not my cup of tea',\n",
        "    'An excellent film',\n",
        "    'I would not recommend this movie',\n",
        "    'Best movie ever'\n",
        "]\n",
        "\n",
        "# Corresponding labels (1 for positive, 0 for negative)\n",
        "labels = [1, 0, 1, 0, 1, 1, 0, 1, 0, 1]\n"
      ],
      "metadata": {
        "id": "f8iBMzZ37L7L"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(texts)\n",
        "\n",
        "# Convert texts to sequences\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=16, input_length=max_length),\n",
        "    GRU(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "3VSbAcJU7RBX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12) How do you implement a text generation model using LSTM with Keras?"
      ],
      "metadata": {
        "id": "dJqDpRFE7Xjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "k1x2N1GK7fl7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your text data\n",
        "text = open('your_text_file.txt', 'r').read().lower()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts([text])\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Define sequence length\n",
        "seq_length = 40\n",
        "step = 3\n",
        "\n",
        "# Create input-output pairs\n",
        "X = []\n",
        "y = []\n",
        "for i in range(0, len(sequences) - seq_length, step):\n",
        "    X.append(sequences[i: i + seq_length])\n",
        "    y.append(sequences[i + seq_length])\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# One-hot encode the output variable\n",
        "y = tf.keras.utils.to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n"
      ],
      "metadata": {
        "id": "CrCLsDJx7zbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=seq_length),\n",
        "    LSTM(128, return_sequences=True),\n",
        "    LSTM(128),\n",
        "    Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, batch_size=128, epochs=20)\n"
      ],
      "metadata": {
        "id": "Ifw343hK76Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "\n",
        "def generate_text(model, tokenizer, seq_length, seed_text, num_chars):\n",
        "    result = []\n",
        "    input_text = seed_text[-seq_length:]\n",
        "    for _ in range(num_chars):\n",
        "        # Convert input text to sequence\n",
        "        input_seq = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        input_seq = pad_sequences([input_seq], maxlen=seq_length, padding='pre')\n",
        "\n",
        "        # Predict next character\n",
        "        predicted = model.predict(input_seq, verbose=0)\n",
        "        predicted_char_index = np.argmax(predicted, axis=-1)[0]\n",
        "        predicted_char = tokenizer.index_word[predicted_char_index]\n",
        "\n",
        "        # Append to result and update input text\n",
        "        result.append(predicted_char)\n",
        "        input_text += predicted_char\n",
        "        input_text = input_text[1:]\n",
        "\n",
        "    return seed_text + ''.join(result)\n",
        "\n",
        "# Example usage\n",
        "seed_text = \"Once upon a time\"\n",
        "generated_text = generate_text(model, tokenizer, seq_length, seed_text, num_chars=100)\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "pHqQ5BXh8LFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13) How do you implement a simple Bi-directional GRU for sequence labeling?"
      ],
      "metadata": {
        "id": "Wg2KOwrp8SLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, GRU, Dense, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "g__BxWnk8VRV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of sequences (e.g., sentences) and their labels (e.g., POS tags)\n",
        "sequences = [\n",
        "    ['I', 'love', 'programming'],\n",
        "    ['Python', 'is', 'awesome'],\n",
        "    ['Keras', 'makes', 'building', 'models', 'easy']\n",
        "]\n",
        "\n",
        "labels = [\n",
        "    ['PRON', 'VERB', 'NOUN'],\n",
        "    ['NOUN', 'VERB', 'ADJ'],\n",
        "    ['NOUN', 'VERB', 'VERB', 'NOUN', 'ADJ']\n",
        "]\n"
      ],
      "metadata": {
        "id": "kjaEc70B8lFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Initialize tokenizers\n",
        "word_tokenizer = Tokenizer()\n",
        "label_tokenizer = Tokenizer()\n",
        "\n",
        "# Fit tokenizers on the data\n",
        "word_tokenizer.fit_on_texts(sequences)\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "# Convert texts to sequences\n",
        "X = word_tokenizer.texts_to_sequences(sequences)\n",
        "y = label_tokenizer.texts_to_sequences(labels)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "max_length = max(len(seq) for seq in X)\n",
        "X = pad_sequences(X, maxlen=max_length, padding='post')\n",
        "y = pad_sequences(y, maxlen=max_length, padding='post')\n",
        "\n",
        "# Convert labels to categorical (one-hot encoding)\n",
        "num_classes = len(label_tokenizer.word_index) + 1\n",
        "y = to_categorical(y, num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "lQUvGLGl87qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word_tokenizer.word_index) + 1, output_dim=64, input_length=max_length),\n",
        "    Bidirectional(GRU(64, return_sequences=True)),\n",
        "    TimeDistributed(Dense(num_classes, activation='softmax'))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "BbhyQuNa8-2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}